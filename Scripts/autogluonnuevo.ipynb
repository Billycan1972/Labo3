{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883eb6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü° Procesando chunks...\n",
      "‚úÖ Chunk 1: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 500000)\n",
      "‚úÖ Chunk 2: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 1000000)\n",
      "‚úÖ Chunk 3: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 1500000)\n",
      "‚úÖ Chunk 4: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 2000000)\n",
      "‚úÖ Chunk 5: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 2500000)\n",
      "‚úÖ Chunk 6: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 3000000)\n",
      "‚úÖ Chunk 7: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 3500000)\n",
      "‚úÖ Chunk 8: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 4000000)\n",
      "‚úÖ Chunk 9: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 4500000)\n",
      "‚úÖ Chunk 10: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 5000000)\n",
      "‚úÖ Chunk 11: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 5500000)\n",
      "‚úÖ Chunk 12: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 6000000)\n",
      "‚úÖ Chunk 13: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 6500000)\n",
      "‚úÖ Chunk 14: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 7000000)\n",
      "‚úÖ Chunk 15: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 7500000)\n",
      "‚úÖ Chunk 16: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 8000000)\n",
      "‚úÖ Chunk 17: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 8500000)\n",
      "‚úÖ Chunk 18: 500000 filas ‚Üí 500000 √∫tiles (acumuladas: 9000000)\n",
      "‚úÖ Chunk 19: 460980 filas ‚Üí 460980 √∫tiles (acumuladas: 9460980)\n",
      "‚úÖ Todos los chunks procesados. Total acumulado: 9460980\n",
      "üì¶ Generando enero sint√©tico...\n",
      "üìù Archivo 'enero_fake.csv' exportado para inspecci√≥n.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\waldo\\AppData\\Local\\Temp\\ipykernel_17916\\3520336710.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_train_con_enero = pd.concat([df_train, enero_fake], ignore_index=True)\n",
      "Beginning AutoGluon training... Time limit = 600s\n",
      "AutoGluon will save models to 'c:\\Users\\waldo\\Dropbox\\Maestr√≠a Ciencia de Datos\\Labo 3\\Proceso\\AutogluonModels\\ag-20250627_002622'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.9.23\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       19.85 GB / 63.68 GB (31.2%)\n",
      "Disk Space Avail:   1352.86 GB / 1862.26 GB (72.6%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': RMSE,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 1,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 600,\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Entrenando modelo con AutoGluon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_data with frequency 'IRREG' has been resampled to frequency 'MS'.\n",
      "Provided train_data has 28860 rows (NaN fraction=2.7%), 780 time series. Median time series length is 37 (min=37, max=37). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-06-26 21:26:40\n",
      "Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\n",
      "Training timeseries model SeasonalNaive. Training for up to 44.7s of the 581.7s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t0.27    s     = Training runtime\n",
      "\t21.49   s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. Training for up to 46.7s of the 559.9s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t41.18   s     = Training runtime\n",
      "\t0.81    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 47.1s of the 517.7s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t8.75    s     = Training runtime\n",
      "\t0.53    s     = Validation (prediction) runtime\n",
      "Training timeseries model NPTS. Training for up to 50.8s of the 508.4s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t0.53    s     = Training runtime\n",
      "\t6.04    s     = Validation (prediction) runtime\n",
      "Training timeseries model DynamicOptimizedTheta. Training for up to 55.8s of the 501.8s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t0.35    s     = Training runtime\n",
      "\t12.97   s     = Validation (prediction) runtime\n",
      "Training timeseries model AutoETS. Training for up to 61.0s of the 488.4s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t0.17    s     = Training runtime\n",
      "\t18.68   s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 67.1s of the 469.5s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t0.13    s     = Training runtime\n",
      "\t16.80   s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 75.4s of the 452.5s of remaining time.\n",
      "\tSkipping covariate_regressor since the dataset contains no covariates or static features.\n",
      "\tSaving fine-tuned model to c:\\Users\\waldo\\Dropbox\\Maestr√≠a Ciencia de Datos\\Labo 3\\Proceso\\AutogluonModels\\ag-20250627_002622\\models\\ChronosFineTuned[bolt_small]\\W0\\fine-tuned-ckpt\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t68.89   s     = Training runtime\n",
      "\t3.34    s     = Validation (prediction) runtime\n",
      "Training timeseries model TemporalFusionTransformer. Training for up to 76.1s of the 380.3s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t70.14   s     = Training runtime\n",
      "\t2.11    s     = Validation (prediction) runtime\n",
      "Training timeseries model DeepAR. Training for up to 77.0s of the 307.9s of remaining time.\n",
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\t[WinError 32] El proceso no tiene acceso al archivo porque est√° siendo utilizado por otro proceso: 'c:\\\\Users\\\\waldo\\\\Dropbox\\\\Maestr√≠a Ciencia de Datos\\\\Labo 3\\\\Proceso\\\\AutogluonModels\\\\ag-20250627_002622\\\\models\\\\DeepAR\\\\W0\\\\lightning_logs\\\\version_0'\n",
      "Training timeseries model PatchTST. Training for up to 79.1s of the 237.4s of remaining time.\n",
      "\tWarning: Exception caused PatchTST to fail during training... Skipping this model.\n",
      "\t[WinError 32] El proceso no tiene acceso al archivo porque est√° siendo utilizado por otro proceso: 'c:\\\\Users\\\\waldo\\\\Dropbox\\\\Maestr√≠a Ciencia de Datos\\\\Labo 3\\\\Proceso\\\\AutogluonModels\\\\ag-20250627_002622\\\\models\\\\PatchTST\\\\W0\\\\lightning_logs\\\\version_0'\n",
      "Training timeseries model TiDE. Training for up to 82.0s of the 164.1s of remaining time.\n",
      "\tnan           = Validation score (-RMSE)\n",
      "\t86.49   s     = Training runtime\n",
      "\t6.45    s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tWarning: Exception caused ensemble to fail during training... Skipping this model.\n",
      "\t'a' cannot be empty unless no samples are taken\n",
      "Training complete. Models trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'TiDE']\n",
      "Total runtime: 511.30 s\n",
      "Best model: SeasonalNaive\n",
      "Best model score: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo entrenado.\n",
      "üîÆ Generando predicciones para febrero 2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data with frequency 'IRREG' has been resampled to frequency 'MS'.\n",
      "Model not specified in predict, will default to the model with the best validation score: SeasonalNaive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Columnas disponibles en predicciones: ['item_id', 'timestamp', 'mean', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']\n",
      "‚úÖ Archivo exportado como 'prediccion_feb2020_autogluon_from_enero.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesPredictor\n",
    "\n",
    "# === 1. Cargar en chunks y filtrar hasta dic 2019 ===\n",
    "usecols = [\"product_id\", \"periodo\", \"tn\"]\n",
    "chunks = pd.read_csv(\"dataset_base_features.csv\",\n",
    "                     usecols=usecols,\n",
    "                     parse_dates=[\"periodo\"],\n",
    "                     chunksize=500_000)\n",
    "\n",
    "filtrados = []\n",
    "total_rows = 0\n",
    "chunk_count = 0\n",
    "\n",
    "print(\"üü° Procesando chunks...\")\n",
    "for chunk in chunks:\n",
    "    chunk_count += 1\n",
    "    original_rows = len(chunk)\n",
    "    chunk = chunk[chunk[\"periodo\"] <= \"2019-12-01\"]\n",
    "    filtrados.append(chunk)\n",
    "    total_rows += len(chunk)\n",
    "    print(f\"‚úÖ Chunk {chunk_count}: {original_rows} filas ‚Üí {len(chunk)} √∫tiles (acumuladas: {total_rows})\")\n",
    "\n",
    "print(\"‚úÖ Todos los chunks procesados. Total acumulado:\", total_rows)\n",
    "\n",
    "# === 2. Renombrar y preparar ===\n",
    "df_train = pd.concat(filtrados, ignore_index=True)\n",
    "df_train = df_train.rename(columns={\"product_id\": \"item_id\", \"periodo\": \"timestamp\", \"tn\": \"target\"})\n",
    "df_train[\"item_id\"] = df_train[\"item_id\"].astype(str)\n",
    "\n",
    "# Forzar timestamps al inicio de mes (por si hay alg√∫n desv√≠o)\n",
    "df_train[\"timestamp\"] = df_train[\"timestamp\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# === 3. Crear enero 2020 sint√©tico duplicando diciembre ===\n",
    "print(\"üì¶ Generando enero sint√©tico...\")\n",
    "enero_fake = df_train[df_train[\"timestamp\"] == \"2019-12-01\"].copy()\n",
    "enero_fake[\"timestamp\"] = pd.to_datetime(\"2020-01-01\")\n",
    "enero_fake[\"target\"] = pd.NA\n",
    "\n",
    "# üîÅ Guardar enero_fake para inspecci√≥n\n",
    "enero_fake.to_csv(\"enero_fake.csv\", index=False)\n",
    "print(\"üìù Archivo 'enero_fake.csv' exportado para inspecci√≥n.\")\n",
    "\n",
    "# === 4. Concatenar enero al dataset ===\n",
    "df_train_con_enero = pd.concat([df_train, enero_fake], ignore_index=True)\n",
    "df_train_con_enero = df_train_con_enero.sort_values([\"item_id\", \"timestamp\"])\n",
    "\n",
    "# === 5. Entrenar modelo ===\n",
    "print(\"‚öôÔ∏è Entrenando modelo con AutoGluon...\")\n",
    "predictor = TimeSeriesPredictor(\n",
    "    target=\"target\",\n",
    "    prediction_length=1,\n",
    "    eval_metric=\"RMSE\",\n",
    "    freq=\"MS\"  # ‚Üê CORRECCI√ìN CLAVE\n",
    ")\n",
    "predictor.fit(train_data=df_train_con_enero, time_limit=600)\n",
    "print(\"‚úÖ Modelo entrenado.\")\n",
    "\n",
    "# === 6. Predecir febrero 2020 ===\n",
    "print(\"üîÆ Generando predicciones para febrero 2020...\")\n",
    "predicciones = predictor.predict(df_train_con_enero)\n",
    "\n",
    "# === 7. Filtrar febrero 2020 ===\n",
    "resultado = predicciones.reset_index()\n",
    "print(\"üß™ Columnas disponibles en predicciones:\", resultado.columns.tolist())\n",
    "\n",
    "# Usamos la columna 'mean' como valor predicho\n",
    "resultado = resultado[resultado[\"timestamp\"] == pd.to_datetime(\"2020-02-01\")]\n",
    "resultado = resultado[[\"item_id\", \"mean\"]].rename(columns={\"item_id\": \"product_id\", \"mean\": \"tn_predicho\"})\n",
    "\n",
    "# === 8. Exportar resultado ===\n",
    "resultado.to_csv(\"prediccion_feb2020_autogluon_from_enero.csv\", index=False)\n",
    "print(\"‚úÖ Archivo exportado como 'prediccion_feb2020_autogluon_from_enero.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
