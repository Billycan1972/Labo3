{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c936588",
   "metadata": {
    "id": "8c936588"
   },
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad9b6a5",
   "metadata": {
    "id": "aad9b6a5",
    "outputId": "0f2ed93f-9d8a-4111-8918-394b66405062"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{x}{2 x^{2} + 3 y^{2} + 1}$"
      ],
      "text/plain": [
       "x/(2*x**2 + 3*y**2 + 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install sympy\n",
    "\n",
    "from sympy import symbols, diff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sympy import lambdify\n",
    "\n",
    "# establece variables a usar en la funcion\n",
    "x, y= symbols('x y', real=True)\n",
    "\n",
    "# define la funcion\n",
    "f = x/(2*x**2 + 3*y*y + 1)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff7d2c",
   "metadata": {
    "id": "2dff7d2c"
   },
   "source": [
    "##### Derivada en X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17513ee9",
   "metadata": {
    "id": "17513ee9",
    "outputId": "05a4132a-9f5f-4f33-dbee-3317a7589e1a"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{4 x^{2}}{\\left(2 x^{2} + 3 y^{2} + 1\\right)^{2}} + \\frac{1}{2 x^{2} + 3 y^{2} + 1}$"
      ],
      "text/plain": [
       "-4*x**2/(2*x**2 + 3*y**2 + 1)**2 + 1/(2*x**2 + 3*y**2 + 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx = diff(f, x)\n",
    "df_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b9ab8e",
   "metadata": {
    "id": "46b9ab8e"
   },
   "source": [
    "##### Derivada en Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ce5686",
   "metadata": {
    "id": "25ce5686",
    "outputId": "1226ea64-464d-468d-ea7a-ac9a9cc2c946"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{6 x y}{\\left(2 x^{2} + 3 y^{2} + 1\\right)^{2}}$"
      ],
      "text/plain": [
       "-6*x*y/(2*x**2 + 3*y**2 + 1)**2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dy = diff(f, y)\n",
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b2f33",
   "metadata": {
    "id": "b60b2f33"
   },
   "source": [
    "##### Verificaci√≥n del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd10add",
   "metadata": {
    "id": "8dd10add",
    "outputId": "77df5afc-1d36-4b4a-8c18-06e69aa90975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dada la funci√≥n f(x,y) =  x/(2*x**2 + 3*y**2 + 1)\n",
      "  en (-0.500000,-1.000000) vale -0.111111\n",
      "\n",
      " - La derivada respecto de x es -4*x**2/(2*x**2 + 3*y**2 + 1)**2 + 1/(2*x**2 + 3*y**2 + 1)\n",
      "   y en (-0.500000,-1.000000) vale 0.172840\n",
      "\n",
      " - La derivada respecto de y es -6*x*y/(2*x**2 + 3*y**2 + 1)**2\n",
      "   y en (-0.500000,-1.000000) vale -0.148148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coordenadas del punto a calcular\n",
    "(px, py) = (-0.5, -1)\n",
    "\n",
    "# evalua la funcion f en x e y\n",
    "pz = f.evalf(subs = {x:px, y:py})\n",
    "print(\"\\nDada la funci√≥n f(x,y) = \", f)\n",
    "print(\"  en (%f,%f) vale %f\\n\" % (px, py, pz) )\n",
    "\n",
    "# evalua la derivada de la funcion f respecto de x en el punto (x e y)\n",
    "pz = df_dx.evalf(subs = {x:px, y:py} )\n",
    "print(\" - La derivada respecto de x es\", df_dx )\n",
    "print(\"   y en (%f,%f) vale %f\\n\" % (px, py, pz) )\n",
    "\n",
    "# evalua la derivada de la funcion f respecto de x en el punto (x e y)\n",
    "pz = df_dy.evalf(subs = {x:px,y:py} )\n",
    "print(\" - La derivada respecto de y es\", df_dy)\n",
    "print(\"   y en (%f,%f) vale %f\\n\" % (px, py, pz) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265064a",
   "metadata": {
    "id": "8265064a"
   },
   "source": [
    "##### Direcci√≥n de crecimiento y decrecimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2d1a51",
   "metadata": {
    "id": "1c2d1a51",
    "outputId": "5aaab230-8d05-4ec9-9f27-cc0e2923f0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La funci√≥n f(x,y) =  x/(2*x**2 + 3*y**2 + 1)\n",
      " en (-0.500000,-1.000000) vale -0.148148\n",
      " en (-0.482716,-1.014815) vale -0.105962 (crece)\n",
      " en (-0.517284,-0.985185) vale -0.116324 (decrece)\n"
     ]
    }
   ],
   "source": [
    "# alfa\n",
    "alfa = 0.1\n",
    "\n",
    "\n",
    "# la funci√≥n crece en direcci√≥n al gradiente\n",
    "rx = px+alfa*df_dx.evalf(subs = {x:px,y:py})\n",
    "ry = py+alfa*df_dy.evalf(subs = {x:px,y:py})\n",
    "rz = f.evalf(subs = {x:rx, y:ry})\n",
    "\n",
    "# la funci√≥n decrece en direcci√≥n al gradiente\n",
    "sx = px-alfa*df_dx.evalf(subs = {x:px,y:py})\n",
    "sy = py-alfa*df_dy.evalf(subs = {x:px,y:py})\n",
    "sz = f.evalf(subs = {x:sx, y:sy})\n",
    "\n",
    "print(\"\\nLa funci√≥n f(x,y) = \", f)\n",
    "print(\" en (%f,%f) vale %f\" % (px, py, pz) )\n",
    "print(\" en (%f,%f) vale %f (crece)\" % (rx, ry, rz) )\n",
    "print(\" en (%f,%f) vale %f (decrece)\" % (sx, sy, sz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66da934",
   "metadata": {
    "id": "b66da934",
    "outputId": "82460708-0321-4bf6-d66d-1153adcf9a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x nuevo: -0.482716\n",
      "y nuevo: -1.014815\n",
      "f(x,y) original: -0.111111111111111\n",
      "f(x,y) nuevo: -0.105961528447659\n",
      "¬øf nueva > f original? True\n",
      "¬øx nueva > x0? True\n",
      "¬øy nueva > y0? False\n"
     ]
    }
   ],
   "source": [
    "# Evaluar la funci√≥n en los puntos original y nuevo\n",
    "# coordenadas del punto a calcular\n",
    "(px, py) = (-0.5, -1)\n",
    "f_original = f.evalf(subs = {x:px, y:py})\n",
    "\n",
    "\n",
    "(px_new, py_new) = (-0.482716,-1.014815)\n",
    "f_nuevo = f.evalf(subs = {x:px_new, y:py_new})\n",
    "\n",
    "# Mostrar resultados y verificaciones\n",
    "print(\"x nuevo:\", px_new)\n",
    "print(\"y nuevo:\", py_new)\n",
    "print(\"f(x,y) original:\", f_original)\n",
    "print(\"f(x,y) nuevo:\", f_nuevo)\n",
    "print(\"¬øf nueva > f original?\", f_nuevo > f_original)\n",
    "print(\"¬øx nueva > x0?\", px_new > px)\n",
    "print(\"¬øy nueva > y0?\", py_new > py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fACLFPHFZ0yG",
   "metadata": {
    "id": "fACLFPHFZ0yG"
   },
   "source": [
    "Analizando el movimiento desde el punto $(-0.5, -1)$ usando el **10% del vector gradiente**, obtenemos:\n",
    "\n",
    "---\n",
    "\n",
    "### üìç Resultados:\n",
    "\n",
    "* **x nuevo** = -0.4827 ‚Üí **mayor que -0.5**\n",
    "* **y nuevo** = -1.0148 ‚Üí **menor que -1**\n",
    "* **f(x, y) nuevo** = -0.10596 ‚Üí **mayor que f(-0.5, -1) = -0.11111**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Afirmaciones verdaderas:\n",
    "\n",
    "* ‚úÖ **El valor de x en la nueva ubicaci√≥n es mayor que -0.5**\n",
    "* ‚úÖ **El valor de y en la nueva ubicaci√≥n es menor que -1**\n",
    "* ‚úÖ **El valor de f(x,y) en la nueva ubicaci√≥n es mayor a f(-0.5, -1)**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Afirmaciones falsas:\n",
    "\n",
    "* ‚ùå El valor de x en la nueva ubicaci√≥n es **menor** que -0.5\n",
    "* ‚ùå El valor de y en la nueva ubicaci√≥n es **mayor** que -1\n",
    "* ‚ùå El valor de f(x,y) en la nueva ubicaci√≥n es **menor** que f(-0.5, -1)\n",
    "\n",
    "¬øQuer√©s que te grafique la funci√≥n y el vector gradiente en ese punto tambi√©n?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236af7d",
   "metadata": {
    "id": "8236af7d"
   },
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b718533",
   "metadata": {
    "id": "9b718533"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# funci√≥n de precio\n",
    "def P(j, k):\n",
    "    return k**2 - 2*j*k + 2*j**2 + 18*j - 20*k + 201\n",
    "\n",
    "# derivadas parciales para el gradiente\n",
    "def dP_dj(j, k):\n",
    "    return -2*k + 4*j + 18\n",
    "\n",
    "def dP_dk(j, k):\n",
    "    return 2*k - 2*j - 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0167c27",
   "metadata": {
    "id": "e0167c27",
    "outputId": "d73c3c7c-4b10-4ddf-8c0f-43afee6e83a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    0.40000    1.60000  177.800\n",
      "   2   -0.09200    2.12800  161.721\n",
      "   3   -0.49328    2.59480  150.005\n",
      "   4   -0.81840    3.00952  141.401\n",
      "   5   -1.07962    3.37984  135.022\n",
      "   6   -1.28727    3.71227  130.236\n",
      "   7   -1.45007    4.01230  126.593\n",
      "   8   -1.57532    4.28456  123.773\n",
      "   9   -1.66921    4.53297  121.548\n",
      "  10   -1.73692    4.76083  119.756\n",
      "  11   -1.78284    4.97097  118.282\n",
      "  12   -1.81064    5.16574  117.042\n",
      "  13   -1.82342    5.34716  115.977\n",
      "  14   -1.82378    5.51692  115.046\n",
      "  15   -1.81391    5.67648  114.216\n",
      "  16   -1.79565    5.82706  113.467\n",
      "  17   -1.77055    5.96969  112.782\n",
      "  18   -1.73990    6.10528  112.150\n",
      "  19   -1.70480    6.23457  111.562\n",
      "  20   -1.66615    6.35821  111.012\n",
      "  21   -1.62472    6.47674  110.494\n",
      "  22   -1.58115    6.59066  110.005\n",
      "  23   -1.53597    6.70035  109.542\n",
      "  24   -1.48963    6.80617  109.103\n",
      "  25   -1.44251    6.90842  108.685\n",
      "  26   -1.39490    7.00737  108.288\n",
      "  27   -1.34707    7.10323  107.910\n",
      "  28   -1.29923    7.19621  107.550\n",
      "  29   -1.25155    7.28649  107.207\n",
      "  30   -1.20417    7.37420  106.879\n",
      "  31   -1.15722    7.45950  106.567\n",
      "  32   -1.11078    7.54250  106.269\n",
      "  33   -1.06494    7.62330  105.985\n",
      "  34   -1.01975    7.70201  105.713\n",
      "  35   -0.97526    7.77870  105.454\n",
      "  36   -0.93151    7.85346  105.207\n",
      "  37   -0.88852    7.92636  104.971\n",
      "  38   -0.84631    7.99747  104.746\n",
      "  39   -0.80491    8.06684  104.531\n",
      "  40   -0.76431    8.13454  104.325\n",
      "  41   -0.72452    8.20061  104.129\n",
      "  42   -0.68554    8.26510  103.942\n",
      "  43   -0.64737    8.32806  103.764\n",
      "  44   -0.61000    8.38954  103.593\n",
      "  45   -0.57343    8.44956  103.430\n",
      "  46   -0.53764    8.50818  103.275\n",
      "  47   -0.50264    8.56544  103.126\n",
      "  48   -0.46839    8.62135  102.985\n",
      "  49   -0.43490    8.67597  102.850\n",
      "  50   -0.40216    8.72931  102.720\n",
      "  51   -0.37014    8.78143  102.597\n",
      "  52   -0.33884    8.83233  102.479\n",
      "  53   -0.30824    8.88206  102.367\n",
      "  54   -0.27833    8.93064  102.260\n",
      "  55   -0.24909    8.97811  102.157\n",
      "  56   -0.22051    9.02447  102.060\n",
      "  57   -0.19258    9.06977  101.966\n",
      "  58   -0.16528    9.11403  101.877\n",
      "  59   -0.13861    9.15727  101.792\n",
      "  60   -0.11254    9.19952  101.711\n",
      "  61   -0.08706    9.24080  101.633\n",
      "  62   -0.06217    9.28113  101.559\n",
      "  63   -0.03784    9.32053  101.489\n",
      "  64   -0.01407    9.35903  101.421\n",
      "  65    0.00916    9.39664  101.357\n",
      "  66    0.03186    9.43339  101.295\n",
      "  67    0.05404    9.46930  101.237\n",
      "  68    0.07571    9.50438  101.181\n",
      "  69    0.09689    9.53866  101.127\n",
      "  70    0.11758    9.57216  101.076\n",
      "  71    0.13780    9.60488  101.027\n",
      "  72    0.15756    9.63686  100.981\n",
      "  73    0.17686    9.66810  100.936\n",
      "  74    0.19573    9.69863  100.894\n",
      "  75    0.21416    9.72845  100.853\n",
      "  76    0.23217    9.75760  100.815\n",
      "  77    0.24976    9.78607  100.778\n",
      "  78    0.26695    9.81389  100.743\n",
      "  79    0.28375    9.84107  100.709\n",
      "  80    0.30017    9.86764  100.677\n",
      "  81    0.31621    9.89359  100.646\n",
      "  82    0.33188    9.91894  100.617\n",
      "  83    0.34719    9.94372  100.589\n",
      "  84    0.36215    9.96793  100.562\n",
      "  85    0.37677    9.99158  100.537\n",
      "  86    0.39105   10.01469  100.512\n",
      "  87    0.40500   10.03727  100.489\n",
      "  88    0.41864   10.05934  100.467\n",
      "  89    0.43196   10.08090  100.446\n",
      "  90    0.44498   10.10196  100.426\n",
      "  91    0.45770   10.12254  100.406\n",
      "  92    0.47013   10.14265  100.388\n",
      "  93    0.48227   10.16230  100.370\n",
      "  94    0.49414   10.18150  100.354\n",
      "  95    0.50573   10.20026  100.338\n",
      "  96    0.51706   10.21859  100.322\n",
      "  97    0.52813   10.23649  100.308\n",
      "  98    0.53894   10.25399  100.294\n",
      "  99    0.54951   10.27109  100.280\n",
      " 100    0.55983   10.28779  100.268\n",
      " 101    0.56992   10.30412  100.256\n",
      " 102    0.57978   10.32006  100.244\n",
      " 103    0.58941   10.33565  100.233\n",
      " 104    0.59882   10.35087  100.222\n",
      " 105    0.60801   10.36575  100.212\n",
      " 106    0.61700   10.38029  100.203\n",
      " 107    0.62577   10.39449  100.194\n",
      " 108    0.63435   10.40836  100.185\n",
      " 109    0.64273   10.42192  100.176\n",
      " 110    0.65092   10.43517  100.168\n",
      " 111    0.65892   10.44812  100.161\n",
      " 112    0.66673   10.46077  100.153\n",
      " 113    0.67437   10.47312  100.147\n",
      " 114    0.68183   10.48520  100.140\n",
      " 115    0.68913   10.49700  100.134\n",
      " 116    0.69625   10.50852  100.128\n",
      " 117    0.70321   10.51979  100.122\n",
      " 118    0.71001   10.53079  100.116\n",
      " 119    0.71666   10.54155  100.111\n",
      " 120    0.72315   10.55205  100.106\n",
      " 121    0.72950   10.56232  100.101\n",
      " 122    0.73570   10.57235  100.097\n",
      " 123    0.74176   10.58215  100.092\n",
      " 124    0.74767   10.59173  100.088\n",
      " 125    0.75346   10.60108  100.084\n",
      " 126    0.75911   10.61023  100.080\n",
      " 127    0.76463   10.61916  100.077\n",
      " 128    0.77002   10.62789  100.073\n",
      " 129    0.77529   10.63642  100.070\n",
      " 130    0.78044   10.64475  100.067\n",
      " 131    0.78547   10.65289  100.064\n",
      " 132    0.79039   10.66084  100.061\n",
      " 133    0.79519   10.66862  100.058\n",
      " 134    0.79989   10.67621  100.055\n",
      " 135    0.80447   10.68363  100.053\n",
      " 136    0.80896   10.69088  100.050\n",
      " 137    0.81333   10.69797  100.048\n",
      " 138    0.81761   10.70489  100.046\n",
      " 139    0.82179   10.71165  100.044\n",
      " 140    0.82588   10.71826  100.042\n",
      " 141    0.82987   10.72472  100.040\n",
      " 142    0.83377   10.73103  100.038\n",
      " 143    0.83758   10.73719  100.036\n",
      " 144    0.84130   10.74321  100.035\n",
      " 145    0.84493   10.74910  100.033\n",
      " 146    0.84849   10.75485  100.032\n",
      " 147    0.85196   10.76047  100.030\n",
      " 148    0.85535   10.76596  100.029\n",
      " 149    0.85867   10.77132  100.028\n",
      " 150    0.86191   10.77656  100.026\n",
      " 151    0.86507   10.78168  100.025\n",
      " 152    0.86816   10.78669  100.024\n",
      " 153    0.87119   10.79157  100.023\n",
      " 154    0.87414   10.79635  100.022\n",
      " 155    0.87702   10.80102  100.021\n",
      " 156    0.87984   10.80558  100.020\n",
      " 157    0.88259   10.81003  100.019\n",
      " 158    0.88529   10.81439  100.018\n",
      " 159    0.88791   10.81864  100.017\n",
      " 160    0.89048   10.82280  100.017\n",
      " 161    0.89299   10.82686  100.016\n",
      " 162    0.89545   10.83083  100.015\n",
      " 163    0.89784   10.83470  100.014\n",
      " 164    0.90018   10.83849  100.014\n",
      " 165    0.90247   10.84219  100.013\n",
      " 166    0.90471   10.84581  100.013\n",
      " 167    0.90689   10.84934  100.012\n",
      " 168    0.90902   10.85280  100.011\n",
      " 169    0.91111   10.85617  100.011\n",
      " 170    0.91315   10.85947  100.010\n",
      " 171    0.91514   10.86269  100.010\n",
      " 172    0.91708   10.86583  100.010\n",
      " 173    0.91898   10.86891  100.009\n",
      " 174    0.92084   10.87191  100.009\n",
      " 175    0.92265   10.87485  100.008\n",
      " 176    0.92443   10.87772  100.008\n",
      " 177    0.92616   10.88052  100.008\n",
      " 178    0.92785   10.88326  100.007\n",
      " 179    0.92950   10.88593  100.007\n",
      " 180    0.93112   10.88855  100.007\n",
      " 181    0.93270   10.89110  100.006\n",
      " 182    0.93424   10.89360  100.006\n",
      " 183    0.93575   10.89604  100.006\n",
      " 184    0.93722   10.89842  100.005\n",
      " 185    0.93866   10.90075  100.005\n",
      " 186    0.94006   10.90302  100.005\n",
      " 187    0.94144   10.90524  100.005\n",
      " 188    0.94278   10.90742  100.005\n",
      " 189    0.94409   10.90954  100.004\n",
      " 190    0.94537   10.91161  100.004\n",
      " 191    0.94662   10.91364  100.004\n",
      " 192    0.94785   10.91562  100.004\n",
      " 193    0.94904   10.91755  100.004\n",
      " 194    0.95021   10.91944  100.003\n",
      " 195    0.95135   10.92129  100.003\n",
      " 196    0.95247   10.92309  100.003\n",
      " 197    0.95356   10.92485  100.003\n",
      " 198    0.95462   10.92657  100.003\n",
      " 199    0.95566   10.92826  100.003\n",
      " 200    0.95668   10.92990  100.003\n",
      " 201    0.95767   10.93151  100.002\n",
      " 202    0.95864   10.93308  100.002\n",
      " 203    0.95959   10.93461  100.002\n",
      " 204    0.96051   10.93611  100.002\n",
      " 205    0.96142   10.93757  100.002\n",
      "alfa: 0.03\n",
      "Valores √≥ptimos: j =  0.96142, k = 10.93757\n",
      "Precio m√≠nimo..: 100002 d√≥lares\n"
     ]
    }
   ],
   "source": [
    "# Algoritmo y par√°metros de descenso de gradiente\n",
    "alfa=[0.03, 0.3, 0.385]\n",
    "max_itera=500\n",
    "cota = 10e-05\n",
    "\n",
    "# j y k son valores inciales cualquiera dentro del dominio y z_new debe ser diferente para entrar al while\n",
    "j_ini, k_ini = 1, 1\n",
    "(j, k, z, z_new) = (j_ini, k_ini, P(j_ini, k_ini), P(j_ini, k_ini)+1)\n",
    "\n",
    "itera=0\n",
    "while itera < max_itera and (abs(z_new - z) > cota):\n",
    "    z = z_new\n",
    "\n",
    "    # Calcula los gradientes\n",
    "    grad_j = dP_dj(j, k)\n",
    "    grad_k = dP_dk(j, k)\n",
    "\n",
    "    # Actualiza los valores de j y k\n",
    "    j = j - alfa[0] * grad_j\n",
    "    k = k - alfa[0] * grad_k\n",
    "    z_new = P(j, k)\n",
    "\n",
    "    itera += 1\n",
    "\n",
    "    print(f\"{itera:>4} {j:>10.5f} {k:>10.5f} {P(j, k):>8.3f}\")\n",
    "print(f\"alfa: {alfa[0]}\")\n",
    "print(f\"Valores √≥ptimos: j = {j:8.5f}, k = {k:8.5f}\")\n",
    "print(f\"Precio m√≠nimo..:{(1000*P(j, k)):7.0f} d√≥lares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8315025",
   "metadata": {
    "id": "e8315025",
    "outputId": "ed84b100-bf69-4782-e1cb-4215590b7b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1   -5.00000    7.00000  140.000\n",
      "   2   -0.20000    5.80000  117.440\n",
      "   3   -1.88000    8.20000  108.301\n",
      "   4   -0.10400    8.15200  104.260\n",
      "   5   -0.48800    9.19840  102.312\n",
      "   6    0.21664    9.38656  101.303\n",
      "   7    0.18861    9.88461  100.751\n",
      "   8    0.49304   10.06701  100.439\n",
      "   9    0.54160   10.32263  100.258\n",
      "  10    0.68526   10.45401  100.153\n",
      "  11    0.73535   10.59276  100.090\n",
      "  12    0.80858   10.67832  100.054\n",
      "  13    0.84527   10.75648  100.032\n",
      "  14    0.88483   10.80975  100.019\n",
      "  15    0.90889   10.85480  100.011\n",
      "  16    0.93110   10.88725  100.007\n",
      "  17    0.94613   10.91356  100.004\n",
      "  18    0.95891   10.93310  100.002\n",
      "  19    0.96808   10.94859  100.001\n",
      "  20    0.97554   10.96028  100.001\n",
      "  21    0.98106   10.96944  100.000\n",
      "  22    0.98545   10.97641  100.000\n",
      "  23    0.98876   10.98183  100.000\n",
      "  24    0.99135   10.98599  100.000\n",
      "alfa: 0.3\n",
      "Valores √≥ptimos: j =  0.99135, k = 10.98599\n",
      "Precio m√≠nimo..: 100000 d√≥lares\n"
     ]
    }
   ],
   "source": [
    "# Algoritmo y par√°metros de descenso de gradiente\n",
    "alfa=[0.03, 0.3, 0.385]\n",
    "max_itera=500\n",
    "cota = 10e-05\n",
    "\n",
    "# j y k son valores inciales cualquiera dentro del dominio y z_new debe ser diferente para entrar al while\n",
    "j_ini, k_ini = 1, 1\n",
    "(j, k, z, z_new) = (j_ini, k_ini, P(j_ini, k_ini), P(j_ini, k_ini)+1)\n",
    "\n",
    "itera=0\n",
    "while itera < max_itera and (abs(z_new - z) > cota):\n",
    "    z = z_new\n",
    "\n",
    "    # Calcula los gradientes\n",
    "    grad_j = dP_dj(j, k)\n",
    "    grad_k = dP_dk(j, k)\n",
    "\n",
    "    # Actualiza los valores de j y k\n",
    "    j = j - alfa[1] * grad_j\n",
    "    k = k - alfa[1] * grad_k\n",
    "    z_new = P(j, k)\n",
    "\n",
    "    itera += 1\n",
    "\n",
    "    print(f\"{itera:>4} {j:>10.5f} {k:>10.5f} {P(j, k):>8.3f}\")\n",
    "print(f\"alfa: {alfa[1]}\")\n",
    "print(f\"Valores √≥ptimos: j = {j:8.5f}, k = {k:8.5f}\")\n",
    "print(f\"Precio m√≠nimo..:{(1000*P(j, k)):7.0f} d√≥lares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47abe16",
   "metadata": {
    "id": "c47abe16",
    "outputId": "c7bece5b-add9-4dc0-9bd5-4fc0add45a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1   -6.70000    8.70000  188.450\n",
      "   2    3.38700    4.54200  183.932\n",
      "   3   -5.26164   11.35265  182.957\n",
      "   4    4.65283    6.25965  183.789\n",
      "   5   -4.62260   12.72239  185.563\n",
      "   6    5.36245    7.06675  187.850\n",
      "   7   -4.38432   13.45444  190.437\n",
      "   8    5.79745    7.41859  193.221\n",
      "   9   -4.34831   13.87031  196.150\n",
      "  10    6.09823    7.54197  199.201\n",
      "  11   -4.41572   14.13029  202.364\n",
      "  12    6.33481    7.54986  205.636\n",
      "  13   -4.53741   14.31427  209.015\n",
      "  14    6.54219    7.49848  212.505\n",
      "  15   -4.68895   14.46214  216.107\n",
      "  16    6.73788    7.41580  219.825\n",
      "  17   -4.85829   14.59380  223.662\n",
      "  18    6.93070    7.31569  227.622\n",
      "  19   -5.03950   14.71925  231.709\n",
      "  20    7.12515    7.20501  235.927\n",
      "  21   -5.22972   14.84352  240.280\n",
      "  22    7.32356    7.08712  244.772\n",
      "  23   -5.42764   14.96918  249.408\n",
      "  24    7.52719    6.96363  254.193\n",
      "  25   -5.63269   15.09757  259.131\n",
      "  26    7.73678    6.83527  264.227\n",
      "  27   -5.84470   15.22944  269.487\n",
      "  28    7.95281    6.70235  274.914\n",
      "  29   -6.06371   15.36520  280.516\n",
      "  30    8.17561    6.56494  286.297\n",
      "  31   -6.28982   15.50515  292.263\n",
      "  32    8.40547    6.42302  298.420\n",
      "  33   -6.52323   15.64951  304.774\n",
      "  34    8.64266    6.27650  311.332\n",
      "  35   -6.76413   15.79845  318.100\n",
      "  36    8.88744    6.12526  325.085\n",
      "  37   -7.01277   15.95214  332.293\n",
      "  38    9.14004    5.96916  339.732\n",
      "  39   -7.26937   16.11074  347.410\n",
      "  40    9.40072    5.80806  355.333\n",
      "  41   -7.53419   16.27441  363.510\n",
      "  42    9.66976    5.64179  371.949\n",
      "  43   -7.80749   16.44333  380.658\n",
      "  44    9.94740    5.47020  389.646\n",
      "  45   -8.08955   16.61765  398.921\n",
      "  46   10.23394    5.29311  408.494\n",
      "  47   -8.38064   16.79755  418.374\n",
      "  48   10.52966    5.11035  428.570\n",
      "  49   -8.68105   16.98322  439.092\n",
      "  50   10.83484    4.92173  449.951\n",
      "  51   -8.99108   17.17483  461.158\n",
      "  52   11.14980    4.72708  472.724\n",
      "  53   -9.31104   17.37257  484.661\n",
      "  54   11.47485    4.52619  496.979\n",
      "  55   -9.64125   17.57665  509.692\n",
      "  56   11.81030    4.31887  522.813\n",
      "  57   -9.98203   17.78727  536.353\n",
      "  58   12.15650    4.10491  550.327\n",
      "  59  -10.33373   18.00463  564.749\n",
      "  60   12.51378    3.88409  579.632\n",
      "  61  -10.69669   18.22895  594.992\n",
      "  62   12.88251    3.65621  610.844\n",
      "  63  -11.07127   18.46046  627.204\n",
      "  64   13.26304    3.42103  644.088\n",
      "  65  -11.45785   18.69938  661.512\n",
      "  66   13.65576    3.17831  679.494\n",
      "  67  -11.85681   18.94595  698.052\n",
      "  68   14.06106    2.92782  717.205\n",
      "  69  -12.26855   19.20041  736.971\n",
      "  70   14.47933    2.66931  757.369\n",
      "  71  -12.69347   19.46303  778.421\n",
      "  72   14.91100    2.40253  800.148\n",
      "  73  -13.13200   19.73405  822.570\n",
      "  74   15.35650    2.12719  845.710\n",
      "  75  -13.58457   20.01376  869.591\n",
      "  76   15.81626    1.84305  894.237\n",
      "  77  -14.05164   20.30242  919.672\n",
      "  78   16.29075    1.54980  945.922\n",
      "  79  -14.53366   20.60033  973.012\n",
      "  80   16.78043    1.24716 1000.970\n",
      "  81  -15.03112   20.90778 1029.823\n",
      "  82   17.28580    0.93482 1059.601\n",
      "  83  -15.54451   21.22507 1090.332\n",
      "  84   17.80734    0.61249 1122.047\n",
      "  85  -16.07435   21.55253 1154.778\n",
      "  86   18.34559    0.27983 1188.557\n",
      "  87  -16.62115   21.89047 1223.417\n",
      "  88   18.90108   -0.06348 1259.394\n",
      "  89  -17.18546   22.23923 1296.524\n",
      "  90   19.47436   -0.41778 1334.842\n",
      "  91  -17.76785   22.59917 1374.387\n",
      "  92   20.06599   -0.78343 1415.199\n",
      "  93  -18.36888   22.97063 1457.318\n",
      "  94   20.67658   -1.16079 1500.786\n",
      "  95  -18.98916   23.35398 1545.646\n",
      "  96   21.30671   -1.55024 1591.942\n",
      "  97  -19.62931   23.74961 1639.721\n",
      "  98   21.95703   -1.95216 1689.030\n",
      "  99  -20.28996   24.15792 1739.918\n",
      " 100   22.62817   -2.36695 1792.436\n",
      " 101  -20.97176   24.57930 1846.636\n",
      " 102   23.32081   -2.79502 1902.572\n",
      " 103  -21.67540   25.01417 1960.299\n",
      " 104   24.03563   -3.23680 2019.874\n",
      " 105  -22.40158   25.46297 2081.357\n",
      " 106   24.77334   -3.69273 2144.810\n",
      " 107  -23.15100   25.92614 2210.294\n",
      " 108   25.53467   -4.16326 2277.876\n",
      " 109  -23.92443   26.40415 2347.622\n",
      " 110   26.32039   -4.64886 2419.601\n",
      " 111  -24.72263   26.89746 2493.886\n",
      " 112   27.13127   -5.15001 2570.549\n",
      " 113  -25.54639   27.40657 2649.668\n",
      " 114   27.96811   -5.66721 2731.320\n",
      " 115  -26.39653   27.93199 2815.588\n",
      " 116   28.83176   -6.20097 2902.553\n",
      " 117  -27.27390   28.47423 2992.304\n",
      " 118   29.72306   -6.75183 3084.930\n",
      " 119  -28.17936   29.03384 3180.521\n",
      " 120   30.64291   -7.32033 3279.174\n",
      " 121  -29.11382   29.61137 3380.986\n",
      " 122   31.59222   -7.90703 3486.059\n",
      " 123  -30.07821   30.20739 3594.496\n",
      " 124   32.57192   -8.51252 3706.407\n",
      " 125  -31.07348   30.82250 3821.901\n",
      " 126   33.58300   -9.13740 3941.093\n",
      " 127  -32.10062   31.45731 4064.103\n",
      " 128   34.62647   -9.78230 4191.053\n",
      " 129  -33.16066   32.11245 4322.067\n",
      " 130   35.70334  -10.44785 4457.278\n",
      " 131  -34.25465   32.78857 4596.819\n",
      " 132   36.81471  -11.13471 4740.828\n",
      " 133  -35.38367   33.48634 4889.449\n",
      " 134   37.96166  -11.84356 5042.830\n",
      " 135  -36.54884   34.20646 5201.122\n",
      " 136   39.14535  -12.57512 5364.484\n",
      " 137  -37.75133   34.94964 5533.078\n",
      " 138   40.36694  -13.33011 5707.071\n",
      " 139  -38.99233   35.71662 5886.636\n",
      " 140   41.62766  -14.10927 6071.951\n",
      " 141  -40.27308   36.50816 6263.201\n",
      " 142   42.92875  -14.91339 6460.576\n",
      " 143  -41.59484   37.32506 6664.271\n",
      " 144   44.27150  -15.74326 6874.490\n",
      " 145  -42.95892   38.16811 7091.441\n",
      " 146   45.65726  -16.59971 7315.340\n",
      " 147  -44.36669   39.03816 7546.410\n",
      " 148   47.08740  -17.48358 7784.879\n",
      " 149  -45.81955   39.93607 8030.985\n",
      " 150   48.56333  -18.39576 8284.972\n",
      " 151  -47.31893   40.86274 8547.094\n",
      " 152   50.08654  -19.33715 8817.609\n",
      " 153  -48.86633   41.81909 9096.788\n",
      " 154   51.65852  -20.30869 9384.908\n",
      " 155  -50.46329   42.80606 9682.254\n",
      " 156   53.28084  -21.31134 9989.123\n",
      " 157  -52.11138   43.82464 10305.820\n",
      " 158   54.95512  -22.34610 10632.658\n",
      " 159  -53.81226   44.87584 10969.963\n",
      " 160   56.68302  -23.41400 11318.071\n",
      " 161  -55.56761   45.96070 11677.326\n",
      " 162   58.46625  -24.51610 12048.087\n",
      " 163  -57.37917   47.08031 12430.721\n",
      " 164   60.30659  -25.65349 12825.609\n",
      " 165  -59.24875   48.23577 13233.143\n",
      " 166   62.20587  -26.82731 13653.728\n",
      " 167  -61.17819   49.42824 14087.782\n",
      " 168   64.16597  -28.03872 14535.737\n",
      " 169  -63.16943   50.65889 14998.037\n",
      " 170   66.18884  -29.28892 15475.143\n",
      " 171  -65.22444   51.92896 15967.527\n",
      " 172   68.27649  -30.57916 16475.680\n",
      " 173  -67.34526   53.23969 17000.107\n",
      " 174   70.43100  -31.91072 17541.328\n",
      " 175  -69.53400   54.59241 18099.881\n",
      " 176   72.65451  -33.28492 18676.322\n",
      " 177  -71.79283   55.98844 19271.224\n",
      " 178   74.94923  -34.70314 19885.176\n",
      " 179  -74.12400   57.42918 20518.791\n",
      " 180   77.31743  -36.16677 21172.697\n",
      " 181  -76.52982   58.91607 21847.544\n",
      " 182   79.76147  -37.67727 22544.003\n",
      " 183  -79.01269   60.45056 23262.766\n",
      " 184   82.28379  -39.23614 24004.547\n",
      " 185  -81.57508   62.03420 24770.083\n",
      " 186   84.88688  -40.84494 25560.135\n",
      " 187  -84.21952   63.66856 26375.489\n",
      " 188   87.57333  -42.50526 27216.954\n",
      " 189  -86.94865   65.35525 28085.366\n",
      " 190   90.34582  -44.21875 28981.590\n",
      " 191  -89.76518   67.09597 29906.514\n",
      " 192   93.20709  -45.98712 30861.059\n",
      " 193  -92.67191   68.89242 31846.174\n",
      " 194   96.16000  -47.81211 32862.836\n",
      " 195  -95.67173   70.74641 33912.056\n",
      " 196   99.20747  -49.69555 34994.877\n",
      " 197  -98.76761   72.65977 36112.376\n",
      " 198  102.35253  -51.63931 37265.662\n",
      " 199 -101.96264   74.63441 38455.882\n",
      " 200  105.59832  -53.64532 39684.218\n",
      " 201 -105.25999   76.67228 40951.891\n",
      " 202  108.94805  -55.71557 42260.161\n",
      " 203 -108.66293   78.77542 43610.328\n",
      " 204  112.40506  -57.85211 45003.734\n",
      " 205 -112.17486   80.94591 46441.763\n",
      " 206  115.97277  -60.05708 47925.845\n",
      " 207 -115.79925   83.18591 49457.454\n",
      " 208  119.65474  -62.33266 51038.112\n",
      " 209 -119.53971   85.49764 52669.390\n",
      " 210  123.45463  -64.68112 54352.910\n",
      " 211 -123.39996   87.88340 56090.344\n",
      " 212  127.37620  -67.10479 57883.418\n",
      " 213 -127.38383   90.34557 59733.916\n",
      " 214  131.42336  -69.60607 61643.675\n",
      " 215 -131.49529   92.88659 63614.593\n",
      " 216  135.60013  -72.18746 65648.630\n",
      " 217 -135.73841   95.50899 67747.806\n",
      " 218  139.91066  -74.85151 69914.208\n",
      " 219 -140.11742   98.21536 72149.988\n",
      " 220  144.35924  -77.60088 74457.368\n",
      " 221 -144.63667  101.00841 76838.641\n",
      " 222  148.95028  -80.43830 79296.174\n",
      " 223 -149.30064  103.89090 81832.409\n",
      " 224  153.68834  -83.36658 84449.866\n",
      " 225 -154.11397  106.86571 87151.146\n",
      " 226  158.57814  -86.38865 89938.934\n",
      " 227 -159.08145  109.93578 92816.000\n",
      " 228  163.62454  -89.50749 95785.204\n",
      " 229 -164.20802  113.10417 98849.495\n",
      " 230  168.83254  -92.72621 102011.919\n",
      " 231 -169.49876  116.37403 105275.619\n",
      " 232  174.20733  -96.04802 108643.838\n",
      " 233 -174.95893  119.74860 112119.923\n",
      " 234  179.75424  -99.47620 115707.329\n",
      " 235 -180.59397  123.23124 119409.621\n",
      " 236  185.47880 -103.01417 123230.477\n",
      " 237 -186.40946  126.82542 127173.695\n",
      " 238  191.38668 -106.66544 131243.194\n",
      " 239 -192.41119  130.53469 135443.017\n",
      " 240  197.48376 -110.43364 139777.338\n",
      " 241 -198.60513  134.36276 144250.465\n",
      " 242  203.77609 -114.32252 148866.842\n",
      " 243 -204.99743  138.31341 153631.058\n",
      " 244  210.26994 -118.33594 158547.846\n",
      " 245 -211.59444  142.39059 163622.093\n",
      " 246  216.97175 -122.47788 168858.841\n",
      " 247 -218.40271  146.59833 174263.294\n",
      " 248  223.88818 -126.75247 179840.824\n",
      " 249 -225.42902  150.94083 185596.972\n",
      " 250  231.02611 -131.16396 191537.460\n",
      " 251 -232.68035  155.42240 197668.189\n",
      " 252  238.39263 -135.71672 203995.254\n",
      " 253 -240.16389  160.04748 210524.941\n",
      " 254  245.99506 -140.41528 217263.739\n",
      " 255 -247.88710  164.82069 224218.346\n",
      " 256  253.84096 -145.26431 231395.671\n",
      " 257 -255.85764  169.74675 238802.849\n",
      " 258  261.93812 -150.26863 246447.239\n",
      " 259 -264.08343  174.83057 254336.439\n",
      " 260  270.29459 -155.43321 262478.288\n",
      " 261 -272.57265  180.07719 270880.877\n",
      " 262  278.91867 -160.76318 279552.557\n",
      " 263 -281.33373  185.49184 288501.945\n",
      " 264  287.81894 -166.26385 297737.935\n",
      " 265 -290.37539  191.07989 307269.704\n",
      " 266  297.00423 -171.94067 317106.726\n",
      " 267 -299.70660  196.84690 327258.775\n",
      " 268  306.48368 -177.79930 337735.941\n",
      " 269 -309.33665  202.79860 348548.635\n",
      " 270  316.26671 -183.84554 359707.603\n",
      " 271 -319.27509  208.94089 371223.933\n",
      " 272  326.36303 -190.08541 383109.071\n",
      " 273 -329.53181  215.27989 395374.826\n",
      " 274  336.78269 -196.52512 408033.389\n",
      " 275 -340.11699  221.82190 421097.339\n",
      " 276  347.53604 -203.17105 434579.658\n",
      " 277 -351.04117  228.57341 448493.744\n",
      " 278  358.63375 -210.02981 462853.425\n",
      " 279 -362.31518  235.54113 477672.971\n",
      " 280  370.08687 -217.10823 492967.108\n",
      " 281 -373.95025  242.73200 508751.036\n",
      " 282  381.90677 -224.41333 525040.440\n",
      " 283 -385.95792  250.15315 541851.507\n",
      " 284  394.10520 -231.95238 559200.943\n",
      " 285 -398.35014  257.81196 577105.991\n",
      " 286  406.69428 -239.73286 595584.442\n",
      " 287 -411.13921  265.71604 614654.660\n",
      " 288  419.68653 -247.76250 634335.597\n",
      " 289 -424.33785  273.87325 654646.810\n",
      " 290  433.09484 -256.04930 675608.484\n",
      " 291 -437.95918  282.29169 697241.449\n",
      " 292  446.93256 -264.60148 719567.204\n",
      " 293 -452.01672  290.97973 742607.935\n",
      " 294  461.21342 -273.42753 766386.539\n",
      " 295 -466.52445  299.94600 790926.645\n",
      " 296  475.95162 -282.53624 816252.642\n",
      " 297 -481.49678  309.19941 842389.696\n",
      " 298  491.16181 -291.93666 869363.782\n",
      " 299 -496.94860  318.74916 897201.705\n",
      " 300  506.85910 -301.63812 925931.130\n",
      " 301 -512.89527  328.60474 955580.606\n",
      " 302  523.05909 -311.65026 986179.598\n",
      " 303 -529.35261  338.77594 1017758.514\n",
      " 304  539.77789 -321.98305 1050348.736\n",
      " 305 -546.33700  349.27287 1083982.651\n",
      " 306  557.03209 -332.64673 1118693.682\n",
      " 307 -563.86532  360.10596 1154516.324\n",
      " 308  574.83886 -343.65192 1191486.175\n",
      " 309 -581.95496  371.28598 1229639.976\n",
      " 310  593.21589 -355.00955 1269015.641\n",
      " 311 -600.62393  382.82404 1309652.300\n",
      " 312  612.18143 -366.73090 1351590.337\n",
      " 313 -619.89076  394.73160 1394871.427\n",
      " 314  631.75434 -378.82762 1439538.582\n",
      " 315 -639.77461  407.02049 1485636.189\n",
      " 316  651.95407 -391.31174 1533210.060\n",
      " 317 -660.29524  419.70293 1582307.470\n",
      " 318  672.80069 -404.19566 1632977.210\n",
      " 319 -681.47303  432.79153 1685269.634\n",
      " 320  694.31491 -417.49218 1739236.708\n",
      " 321 -703.32903  446.29928 1794932.062\n",
      " 322  716.51812 -431.21452 1852411.044\n",
      " 323 -725.88496  460.23961 1911730.774\n",
      " 324  739.43238 -445.37631 1972950.201\n",
      " 325 -749.16325  474.62638 2036130.163\n",
      " 326  763.08047 -459.99163 2101333.445\n",
      " 327 -773.18701  489.47389 2168624.843\n",
      " 328  787.48588 -475.07500 2238071.229\n",
      " 329 -797.98013  504.79687 2309741.615\n",
      " 330  812.67286 -490.64142 2383707.226\n",
      " 331 -823.56724  520.61058 2460041.563\n",
      " 332  838.66645 -506.70634 2538820.486\n",
      " 333 -849.97376  536.93071 2620122.281\n",
      " 334  865.49248 -523.28574 2704027.743\n",
      " 335 -877.22596  553.77349 2790620.253\n",
      " 336  893.17760 -540.39608 2879985.864\n",
      " 337 -905.35089  571.15566 2972213.383\n",
      " 338  921.74934 -558.05438 3067394.461\n",
      " 339 -934.37652  589.09448 3165623.686\n",
      " 340  951.23607 -576.27819 3266998.674\n",
      " 341 -964.33168  607.60779 3371620.167\n",
      " 342  981.66711 -595.08560 3479592.133\n",
      " 343 -995.24615  626.71398 3591021.870\n",
      " 344 1013.07269 -614.49532 3706020.113\n",
      " 345 -1027.15065  646.43205 3824701.142\n",
      " 346 1045.48403 -634.52663 3947182.896\n",
      " 347 -1060.07688  666.78158 4073587.093\n",
      " 348 1078.93333 -655.19943 4204039.349\n",
      " 349 -1094.05756  687.78279 4338669.300\n",
      " 350 1113.45383 -676.53428 4477610.738\n",
      " 351 -1129.12647  709.45657 4621001.734\n",
      " 352 1149.07985 -698.55237 4768984.787\n",
      " 353 -1165.31844  731.82444 4921706.954\n",
      " 354 1185.84678 -721.27558 5079320.005\n",
      " 355 -1202.66945  754.90863 5241980.568\n",
      " 356 1223.79115 -744.72649 5409850.290\n",
      " 357 -1241.21662  778.73209 5583095.991\n",
      " 358 1262.95069 -768.92842 5761889.836\n",
      " 359 -1280.99825  803.31849 5946409.502\n",
      " 360 1303.36430 -793.90540 6136838.358\n",
      " 361 -1322.05388  828.69227 6333365.643\n",
      " 362 1345.07214 -819.68227 6536186.659\n",
      " 363 -1364.42430  854.87863 6745502.959\n",
      " 364 1388.11567 -846.28463 6961522.553\n",
      " 365 -1408.15162  881.90360 7184460.113\n",
      " 366 1432.53765 -873.73892 7414537.185\n",
      " 367 -1453.27930  909.79404 7651982.409\n",
      " 368 1478.38223 -902.07243 7897031.748\n",
      " 369 -1499.85218  938.57766 8149928.721\n",
      " 370 1525.69497 -931.31332 8410924.648\n",
      " 371 -1547.91654  968.28307 8680278.895\n",
      " 372 1574.52289 -961.49063 8958259.133\n",
      " 373 -1597.52015  998.93978 9245141.610\n",
      " 374 1624.91451 -992.63436 9541211.415\n",
      " 375 -1648.71229 1030.57827 9846762.771\n",
      " 376 1676.91991 -1024.77546 10162099.321\n",
      " 377 -1701.54386 1063.22997 10487534.434\n",
      " 378 1730.59076 -1057.94588 10823391.513\n",
      " 379 -1756.06734 1096.92733 11170004.319\n",
      " 380 1785.98041 -1092.17856 11527717.300\n",
      " 381 -1812.33691 1131.70385 11896885.937\n",
      " 382 1843.14389 -1127.50754 12277877.094\n",
      " 383 -1870.40851 1167.59406 12671069.383\n",
      " 384 1902.13802 -1163.96792 13076853.542\n",
      " 385 -1930.33983 1204.63366 13495632.823\n",
      " 386 1963.02142 -1201.59593 13927823.390\n",
      " 387 -1992.19043 1242.85943 14373854.736\n",
      " 388 2025.85460 -1240.42896 14834170.108\n",
      " 389 -2056.02178 1282.30938 15309226.947\n",
      " 390 2090.69998 -1280.50562 15799497.346\n",
      " 391 -2121.89732 1323.02270 16305468.513\n",
      " 392 2157.62203 -1321.86571 16827643.262\n",
      " 393 -2189.88249 1365.03985 17366540.507\n",
      " 394 2226.68723 -1364.55036 17922695.782\n",
      " 395 -2260.04488 1408.40258 18496661.771\n",
      " 396 2297.96422 -1408.60196 19089008.855\n",
      " 397 -2332.45419 1453.15400 19700325.685\n",
      " 398 2371.52384 -1454.06431 20331219.760\n",
      " 399 -2407.18239 1499.33857 20982318.038\n",
      " 400 2447.43919 -1500.98257 21654267.551\n",
      " 401 -2484.30374 1547.00219 22347736.055\n",
      " 402 2525.78570 -1549.40338 23063412.688\n",
      " 403 -2563.89488 1596.19221 23802008.661\n",
      " 404 2606.64124 -1599.37485 24564257.957\n",
      " 405 -2646.03490 1646.95754 25350918.068\n",
      " 406 2690.08615 -1650.94664 26162770.744\n",
      " 407 -2730.80544 1699.34861 27000622.769\n",
      " 408 2776.20337 -1704.17001 27865306.765\n",
      " 409 -2818.29072 1753.41749 28757682.017\n",
      " 410 2865.07846 -1759.09783 29678635.331\n",
      " 411 -2908.57770 1809.21791 30629081.911\n",
      " 412 2956.79975 -1815.78471 31609966.270\n",
      " 413 -3001.75609 1866.80532 32622263.169\n",
      " 414 3051.45839 -1874.28697 33666978.585\n",
      " 415 -3097.91849 1926.23696 34745150.713\n",
      " 416 3149.14844 -1934.66274 35857850.995\n",
      " 417 -3197.16047 1987.57187 37006185.183\n",
      " 418 3249.96699 -1996.97203 38191294.445\n",
      " 419 -3299.58064 2050.87102 39414356.490\n",
      " 420 3354.01423 -2061.27676 40676586.747\n",
      " 421 -3405.28079 2116.19730 41979239.566\n",
      " 422 3461.39355 -2127.64083 43323609.467\n",
      " 423 -3514.36595 2183.61564 44711032.429\n",
      " 424 3572.21166 -2196.13019 46142887.213\n",
      " 425 -3626.94454 2253.19304 47620596.736\n",
      " 426 3686.57869 -2266.81290 49145629.482\n",
      " 427 -3743.12842 2324.99862 50719500.964\n",
      " 428 3804.60829 -2339.75920 52343775.230\n",
      " 429 -3863.03306 2399.10377 54020066.412\n",
      " 430 3926.41775 -2415.04159 55750040.339\n",
      " 431 -3986.77761 2475.58210 57535416.185\n",
      " 432 4052.12813 -2492.73488 59377968.179\n",
      " 433 -4114.48505 2554.50964 61279527.373\n",
      " 434 4181.86435 -2572.91627 63241983.455\n",
      " 435 -4246.28227 2635.96481 65267286.629\n",
      " 436 4315.75533 -2655.66545 67357449.557\n",
      " 437 -4382.30027 2720.02855 69514549.352\n",
      " 438 4453.93413 -2741.06464 71740729.650\n",
      " 439 -4522.67420 2806.78441 74038202.733\n",
      " 440 4596.53807 -2829.19872 76409251.733\n",
      " 441 -4667.54357 2896.31861 78856232.897\n",
      " 442 4743.70886 -2920.15527 81381577.930\n",
      " 443 -4817.05234 2988.72011 83987796.414\n",
      " 444 4895.59275 -3014.02468 86677478.296\n",
      " 445 -4971.34909 3084.08074 89453296.470\n",
      " 446 5052.34067 -3110.90023 92318009.424\n",
      " 447 -5130.58714 3182.49527 95274463.989\n",
      " 448 5214.10841 -3210.87818 98325598.162\n",
      " 449 -5294.92474 3284.06149 101474444.033\n",
      " 450 5381.05671 -3314.05791 104724130.789\n",
      " 451 -5464.52521 3388.88035 108077887.831\n",
      " 452 5553.35148 -3420.54193 111539047.981\n",
      " 453 -5639.55709 3497.05600 115111050.791\n",
      " 454 5731.16395 -3530.43608 118797445.966\n",
      " 455 -5820.19431 3608.69594 122601896.890\n",
      " 456 5914.67080 -3643.84956 126528184.262\n",
      " 457 -6006.61639 3723.91112 130580209.862\n",
      " 458 6104.05442 -3760.89506 134762000.419\n",
      " 459 -6199.00858 3842.81604 139077711.618\n",
      " 460 6299.50298 -3881.68892 143531632.231\n",
      " 461 -6397.56208 3965.52884 148128188.374\n",
      " 462 6501.21073 -4006.35117 152871947.908\n",
      " 463 -6602.47419 4092.17150 157767624.981\n",
      " 464 6709.37812 -4135.00569 162820084.707\n",
      " 465 -6813.94856 4222.86984 168034348.007\n",
      " 466 6924.21200 -4267.78033 173415596.592\n",
      " 467 -7032.19533 4357.75377 178969178.120\n",
      " 468 7145.92588 -4404.80704 184700611.504\n",
      " 469 -7257.43140 4496.95731 190615592.397\n",
      " 470 7374.74008 -4546.22199 196719998.856\n",
      " 471 -7489.88058 4640.61880 203019897.181\n",
      " 472 7610.88199 -4692.16572 209521547.942\n",
      " 473 -7729.77388 4788.88102 216231412.204\n",
      " 474 7854.58628 -4842.78325 223156157.943\n",
      " 475 -7977.34970 4941.89129 230302666.679\n",
      " 476 8106.09513 -4998.22427 237678040.306\n",
      " 477 -8232.85406 5099.80167 245289608.158\n",
      " 478 8365.65847 -5158.64324 253144934.287\n",
      " 479 -8496.54087 5262.76908 261251824.981\n",
      " 480 8633.53426 -5324.19958 269618336.525\n",
      " 481 -8768.67218 5430.95548 278252783.200\n",
      " 482 8909.98869 -5495.05782 287163745.553\n",
      " 483 -9049.51842 5604.52800 296360078.918\n",
      " 484 9195.29650 -5671.38774 305850922.222\n",
      " 485 -9339.35867 5783.65913 315645707.059\n",
      " 486 9489.74121 -5853.36458 325754167.071\n",
      " 487 -9638.48098 5968.52688 336186347.615\n",
      " 488 9793.61542 -6041.16917 346952615.748\n",
      " 489 -9947.18259 6159.31497 358063670.529\n",
      " 490 10107.22112 -6234.98815 369530553.652\n",
      " 491 -10265.77028 6356.21299 381364660.418\n",
      " 492 10430.86996 -6435.01413 393577751.057\n",
      " 493 -10594.56066 6559.41662 406181962.421\n",
      " 494 10764.88355 -6641.44588 419189820.037\n",
      " 495 -10933.88045 6769.12778 432614250.562\n",
      " 496 11109.59383 -6854.48856 446468594.623\n",
      " 497 -11284.06686 6985.55488 460766620.078\n",
      " 498 11465.34336 -7074.35386 475522535.696\n",
      " 499 -11645.46789 7208.91300 490751005.279\n",
      " 500 11832.48567 -7301.26028 506467162.231\n",
      "alfa: 0.385\n",
      "Valores √≥ptimos: j = 11832.48567, k = -7301.26028\n",
      "Precio m√≠nimo..:506467162231 d√≥lares\n"
     ]
    }
   ],
   "source": [
    "# Algoritmo y par√°metros de descenso de gradiente\n",
    "alfa=[0.03, 0.3, 0.385]\n",
    "max_itera=500\n",
    "cota = 10e-05\n",
    "\n",
    "# j y k son valores inciales cualquiera dentro del dominio y z_new debe ser diferente para entrar al while\n",
    "j_ini, k_ini = 1, 1\n",
    "(j, k, z, z_new) = (j_ini, k_ini, P(j_ini, k_ini), P(j_ini, k_ini)+1)\n",
    "\n",
    "itera=0\n",
    "while itera < max_itera and (abs(z_new - z) > cota):\n",
    "    z = z_new\n",
    "\n",
    "    # Calcula los gradientes\n",
    "    grad_j = dP_dj(j, k)\n",
    "    grad_k = dP_dk(j, k)\n",
    "\n",
    "    # Actualiza los valores de j y k\n",
    "    j = j - alfa[2] * grad_j\n",
    "    k = k - alfa[2] * grad_k\n",
    "    z_new = P(j, k)\n",
    "\n",
    "    itera += 1\n",
    "\n",
    "    print(f\"{itera:>4} {j:>10.5f} {k:>10.5f} {P(j, k):>8.3f}\")\n",
    "print(f\"alfa: {alfa[2]}\")\n",
    "print(f\"Valores √≥ptimos: j = {j:8.5f}, k = {k:8.5f}\")\n",
    "print(f\"Precio m√≠nimo..:{(1000*P(j, k)):7.0f} d√≥lares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DgBbtZuKZ-Vz",
   "metadata": {
    "id": "DgBbtZuKZ-Vz"
   },
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TuAVd9xraDIN",
   "metadata": {
    "id": "TuAVd9xraDIN"
   },
   "source": [
    "‚úÖ **Verdadero.**\n",
    "\n",
    "El **vector gradiente** indica:\n",
    "\n",
    "* **Direcci√≥n**: hacia d√≥nde debe moverse el vector de pesos para reducir (o aumentar, si se desea maximizar) la funci√≥n de costo.\n",
    "* **Magnitud**: cu√°nto se deben ajustar los pesos en esa direcci√≥n.\n",
    "\n",
    "En descenso del gradiente cl√°sico o estoc√°stico:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\cdot \\nabla J(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "* Si el gradiente es positivo, el peso disminuye.\n",
    "* Si el gradiente es negativo, el peso aumenta.\n",
    "\n",
    "As√≠ que efectivamente, **el vector gradiente determina tanto la direcci√≥n (suma o resta) como la magnitud del cambio** en los pesos $\\mathbf{w}$ y en el sesgo $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1xainOGacOL",
   "metadata": {
    "id": "b1xainOGacOL"
   },
   "source": [
    "# Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5XNNe4OaeDF",
   "metadata": {
    "id": "M5XNNe4OaeDF"
   },
   "source": [
    "‚úÖ **Opci√≥n correcta:**\n",
    "**\"La diferencia de dos ECM consecutivos durante el entrenamiento alcanza un valor inferior a una cota definida inicialmente.\"**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n:\n",
    "\n",
    "Durante el entrenamiento de un combinador lineal, especialmente cuando se usa descenso del gradiente, **no siempre se alcanza un ECM exactamente igual a cero**, ni tampoco es conveniente esperarlo.\n",
    "\n",
    "Por eso, el criterio de parada m√°s com√∫n y robusto es:\n",
    "\n",
    "> üëâ **Cuando el cambio entre dos errores consecutivos (ECM‚Çñ - ECM‚Çñ‚Çã‚ÇÅ)** es **menor que una tolerancia (Œµ)** predefinida, se considera que el modelo ha **convergido**.\n",
    "\n",
    "Esto evita sobreentrenamiento y acelera el proceso si el modelo ya est√° suficientemente cerca de un m√≠nimo.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Las otras opciones no son correctas:\n",
    "\n",
    "* ‚ùå *\"El ECM llega a cero\"* ‚Üí poco probable, especialmente con datos reales ruidosos.\n",
    "* ‚ùå *\"El ECM alcanza un valor inferior a una cota\"* ‚Üí puede usarse, pero es menos general y m√°s dependiente del problema.\n",
    "* ‚ùå *\"La diferencia llega a cero\"* ‚Üí muy raro con precisi√≥n finita.\n",
    "\n",
    "---\n",
    "\n",
    "¬øQuer√©s que lo veamos implementado en c√≥digo con un gr√°fico de convergencia?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ZL-tEfgahal",
   "metadata": {
    "id": "5ZL-tEfgahal"
   },
   "source": [
    "# Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uc2Ju0Vbazum",
   "metadata": {
    "id": "Uc2Ju0Vbazum"
   },
   "source": [
    "‚ùå **Falso.**\n",
    "\n",
    "Un **combinador lineal** solo puede capturar **relaciones lineales** entre las variables de entrada y la salida. Su forma general es:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Esto representa un **hiperplano lineal** en el espacio de entrada.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç ¬øY relaciones no lineales?\n",
    "\n",
    "Para que un modelo pueda capturar relaciones **no lineales**, se necesita:\n",
    "\n",
    "* **Transformaciones no lineales de las entradas** (por ejemplo, $x^2$, $\\sin(x)$, etc.)\n",
    "* **Modelos no lineales**, como redes neuronales con funciones de activaci√≥n no lineales, √°rboles de decisi√≥n, o SVM con kernels.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Ejemplo:\n",
    "\n",
    "Si el patr√≥n real es $y = x^2$, un combinador lineal no lo podr√° aprender **a menos que** se incluya $x^2$ como una entrada transformada.\n",
    "\n",
    "---\n",
    "\n",
    "¬øQuer√©s que te muestre un ejemplo pr√°ctico en Python comparando ambos casos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ASSKlg0ea2J3",
   "metadata": {
    "id": "ASSKlg0ea2J3"
   },
   "source": [
    "# Ejercicio 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bmc0NnyQcbEu",
   "metadata": {
    "id": "bmc0NnyQcbEu"
   },
   "outputs": [],
   "source": [
    "# Reimportar librer√≠as tras reinicio\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Datos\n",
    "X = np.array([2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 10, 10])\n",
    "Y = np.array([30.5, 29.3, 26.7, 26.0, 25.5, 24.7, 23.5, 23.0, 22.5, 21.0, 21.0, 20.5])\n",
    "\n",
    "# Calcular coeficiente de correlaci√≥n de Pearson\n",
    "correlacion, _ = pearsonr(X, Y)\n",
    "correlacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E72U2DzSccju",
   "metadata": {
    "id": "E72U2DzSccju"
   },
   "source": [
    "El **coeficiente de correlaci√≥n de Pearson** entre las variables es aproximadamente **-0.96**, lo cual indica una:\n",
    "\n",
    "‚úÖ **Correlaci√≥n lineal fuerte y negativa.**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Respuesta correcta:\n",
    "\n",
    "**\"Las variables 'Horas de Entrenamiento' y 'Tiempo Promedio' poseen una correlaci√≥n lineal fuerte.\"**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Interpretaci√≥n:\n",
    "\n",
    "* A medida que aumentan las horas de entrenamiento, el tiempo promedio del recorrido disminuye.\n",
    "* La relaci√≥n es casi perfectamente lineal (pero negativa).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cuMZYKnhioOE",
   "metadata": {
    "id": "cuMZYKnhioOE"
   },
   "source": [
    "# Ejercicio 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wFX4fGFcipsr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1749218429856,
     "user": {
      "displayName": "ivan parra",
      "userId": "10753792741774747128"
     },
     "user_tz": 180
    },
    "id": "wFX4fGFcipsr",
    "outputId": "d4b00a98-4311-4899-d33a-e44a9a425c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 1 ‚Üí ECM: 1.5000\n",
      "Modelo 2 ‚Üí ECM: 0.8038\n",
      "Modelo 3 ‚Üí ECM: 3.9093\n",
      "\n",
      "‚û°Ô∏è Mejor modelo: Modelo 2\n",
      "\n",
      "‚è±Ô∏è Tiempo estimado para 6 horas semanales: 24.43 minutos\n",
      "‚úîÔ∏è Opci√≥n correcta: entre 24 y 25 minutos\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Datos: horas de entrenamiento (X) y tiempos promedio (Y)\n",
    "X = np.array([2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 10, 10])\n",
    "Y = np.array([30.5, 29.3, 26.7, 26.0, 25.5, 24.7, 23.5, 23.0, 22.5, 21.0, 21.0, 20.5])\n",
    "\n",
    "# Definir los tres combinadores lineales\n",
    "modelos = {\n",
    "    \"Modelo 1\": {\"w\": -0.9013, \"b\": 29.5201},\n",
    "    \"Modelo 2\": {\"w\": -1.1301, \"b\": 31.2102},\n",
    "    \"Modelo 3\": {\"w\": -0.7080, \"b\": 30.0401}\n",
    "}\n",
    "\n",
    "# Calcular el ECM de cada modelo\n",
    "ecms = {}\n",
    "for nombre, params in modelos.items():\n",
    "    y_pred = params[\"w\"] * X + params[\"b\"]\n",
    "    ecm = mean_squared_error(Y, y_pred)\n",
    "    ecms[nombre] = ecm\n",
    "    print(f\"{nombre} ‚Üí ECM: {ecm:.4f}\")\n",
    "\n",
    "# Determinar el mejor modelo (menor ECM)\n",
    "mejor_modelo_nombre = min(ecms, key=ecms.get)\n",
    "mejor_modelo = modelos[mejor_modelo_nombre]\n",
    "\n",
    "print(f\"\\n‚û°Ô∏è Mejor modelo: {mejor_modelo_nombre}\")\n",
    "\n",
    "# Predecir el tiempo de recorrido para 6 horas de entrenamiento\n",
    "horas_entrenamiento = 6\n",
    "tiempo_predicho = mejor_modelo[\"w\"] * horas_entrenamiento + mejor_modelo[\"b\"]\n",
    "print(f\"\\n‚è±Ô∏è Tiempo estimado para 6 horas semanales: {tiempo_predicho:.2f} minutos\")\n",
    "\n",
    "# Interpretar el resultado seg√∫n las opciones\n",
    "if tiempo_predicho < 24:\n",
    "    print(\"‚úîÔ∏è Opci√≥n correcta: menos de 24 minutos\")\n",
    "elif 24 <= tiempo_predicho <= 25:\n",
    "    print(\"‚úîÔ∏è Opci√≥n correcta: entre 24 y 25 minutos\")\n",
    "else:\n",
    "    print(\"‚úîÔ∏è Opci√≥n correcta: m√°s de 25 minutos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E1oGYtzujrwm",
   "metadata": {
    "id": "E1oGYtzujrwm"
   },
   "source": [
    "# Ejercicio 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ls6dIjtajvQn",
   "metadata": {
    "id": "Ls6dIjtajvQn"
   },
   "source": [
    "‚úÖ **Verdadero.**\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Justificaci√≥n:\n",
    "\n",
    "La conversi√≥n de **grados Celsius a Fahrenheit** se define por la f√≥rmula:\n",
    "\n",
    "$$\n",
    "F = 1.8 \\cdot C + 32\n",
    "$$\n",
    "\n",
    "Esto es una **funci√≥n lineal con bias**. Tiene:\n",
    "\n",
    "* **Pendiente**: $W = 1.8$\n",
    "* **Bias (intercepto)**: $b = 32$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Si se omite el bias:\n",
    "\n",
    "La f√≥rmula quedar√≠a:\n",
    "\n",
    "$$\n",
    "F = 1.8 \\cdot C\n",
    "$$\n",
    "\n",
    "Esto no cumple la conversi√≥n correctamente (por ejemplo, $0^\\circ C = 32^\\circ F$, pero con bias omitido dar√≠a $0$).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> Un **combinador lineal sin bias no puede resolver correctamente** la conversi√≥n Celsius ‚Üí Fahrenheit.\n",
    "\n",
    "Por eso, la afirmaci√≥n:\n",
    "\n",
    "> **‚ÄúSi se omite el bias, el combinador lineal no resuelve el problema‚Äù**\n",
    "> es **VERDADERA**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xbf578jPlQho",
   "metadata": {
    "id": "xbf578jPlQho"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aEwS1yk8lZ2V",
   "metadata": {
    "id": "aEwS1yk8lZ2V"
   },
   "source": [
    "# Ejercicio 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BiU3LJZ_lcS3",
   "metadata": {
    "id": "BiU3LJZ_lcS3"
   },
   "source": [
    "‚úÖ **Verdadero.**\n",
    "\n",
    "La tabla contiene **solamente 2 atributos num√©ricos con valores faltantes**:\n",
    "\n",
    "* `horsepower` ‚Üí 2 valores faltantes\n",
    "* `price` ‚Üí 4 valores faltantes\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Conclusi√≥n:\n",
    "\n",
    "La afirmaci√≥n:\n",
    "\n",
    "> **‚ÄúLa tabla contiene s√≥lo 3 atributos num√©ricos con valores faltantes‚Äù**\n",
    "> es **‚ùå Falsa**, ya que **s√≥lo hay 2** atributos num√©ricos con valores faltantes, no 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MMJ2qbjRltG1",
   "metadata": {
    "id": "MMJ2qbjRltG1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv(\"automobile-simple.csv\")  # Asegurate que el path sea correcto\n",
    "\n",
    "# Seleccionar solo las columnas num√©ricas\n",
    "atributos_numericos = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Contar cu√°ntos valores faltantes hay en cada atributo num√©rico\n",
    "valores_faltantes = atributos_numericos.isnull().sum()\n",
    "\n",
    "# Filtrar los que tienen al menos un valor faltante\n",
    "atributos_con_faltantes = valores_faltantes[valores_faltantes > 0]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Atributos num√©ricos con valores faltantes:\")\n",
    "print(atributos_con_faltantes)\n",
    "print(f\"\\nCantidad total de atributos num√©ricos con valores faltantes: {len(atributos_con_faltantes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_e17W6muouy3",
   "metadata": {
    "id": "_e17W6muouy3"
   },
   "source": [
    "# Ejercicio 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HM1hGoVjo0-_",
   "metadata": {
    "id": "HM1hGoVjo0-_"
   },
   "outputs": [],
   "source": [
    "def entorno_de_trabajo():\n",
    "    if 'google.colab' in str(get_ipython()): return \"colab\"  # Colab?\n",
    "    else: return \"local\"  # Local => Linux, Windows, WSL\n",
    "\n",
    "if entorno_de_trabajo() == 'colab': # maquina virtual colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')  # monta G-drive en entorno COLAB\n",
    "\n",
    "    FUENTES_DIR = '/content/drive/MyDrive/Colab Notebooks/FUENTES/'  # carpeta donde se encuentran archivos .py auxiliares\n",
    "    DATOS_DIR = '/content/drive/MyDrive/Colab Notebooks/DATOS/'      # carpeta donde se encuentran los datasets\n",
    "else: # entorno local # Rutas relativas al script para entornos locales Windows, WSL y Linux\n",
    "    FUENTES_DIR = '../../../FUENTES/'  # Ajusta seg√∫n tu estructura\n",
    "    DATOS_DIR = '../../../DATOS/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(FUENTES_DIR) # agrega ruta de busqueda donde tenemos archivos .py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gE3_QebVo4Uj",
   "metadata": {
    "id": "gE3_QebVo4Uj"
   },
   "source": [
    "##### Carga del dataset y selecci√≥n de atributos num√©ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEbqw0Jjo7AQ",
   "metadata": {
    "id": "lEbqw0Jjo7AQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from ClassNeuronaLineal import NeuronaLineal\n",
    "\n",
    "nombre_archivo = DATOS_DIR + 'automobile-simple.csv'\n",
    "\n",
    "#-- detectando la codificaci√≥n de caracteres usada ----\n",
    "with open(nombre_archivo, 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "df= pd.read_csv(nombre_archivo, encoding=result['encoding'])\n",
    "\n",
    "print(result['encoding'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-qi6y3-Qo_F0",
   "metadata": {
    "id": "-qi6y3-Qo_F0"
   },
   "source": [
    "##### Asignar atributos sin valor o con valor nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zv_xzT0YpCUW",
   "metadata": {
    "id": "Zv_xzT0YpCUW"
   },
   "outputs": [],
   "source": [
    "# muestra cantidad de valores faltantes o nulos por atributo/caracter√≠stica\n",
    "print('Antes:\\n',df.isnull().sum(), '\\n')\n",
    "\n",
    "values = {'price': df['price'].mean()}\n",
    "\n",
    "# reemplaza valores nulos por los indicados en el diccionario values\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "# comprobacion\n",
    "print('Despues:\\n', df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZMAIOP7KpEfh",
   "metadata": {
    "id": "ZMAIOP7KpEfh"
   },
   "source": [
    "#### Normalizaci√≥n de valores de atributos y valor a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LMWDfEHvpHHP",
   "metadata": {
    "id": "LMWDfEHvpHHP"
   },
   "outputs": [],
   "source": [
    "# Imprime matriz de correclacion\n",
    "corr = df[['engine-size', 'price']].corr()\n",
    "display(corr)\n",
    "\n",
    "# seleccion de atributos\n",
    "X = np.array(df['engine-size']).reshape((-1,1))\n",
    "T = np.array(df['price']).reshape((-1,1))\n",
    "\n",
    "# normalizacion de datos de entrada\n",
    "normalizarEntrada = 0\n",
    "\n",
    "if normalizarEntrada==0:\n",
    "    data_scaler = None\n",
    "    targer_scaler = None\n",
    "elif normalizarEntrada==1:\n",
    "    data_scaler , targer_scaler= MinMaxScaler(), MinMaxScaler()\n",
    "    X = data_scaler.fit_transform(X)\n",
    "    T = targer_scaler.fit_transform(T)\n",
    "elif normalizarEntrada==2:\n",
    "    data_scaler, targer_scaler = StandardScaler(), StandardScaler()\n",
    "    X = data_scaler.fit_transform(X)\n",
    "    T = targer_scaler.fit_transform(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HVmgzm9zpJnV",
   "metadata": {
    "id": "HVmgzm9zpJnV"
   },
   "source": [
    "##### Construcccion del modelo y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RNw9uRE9pMIN",
   "metadata": {
    "id": "RNw9uRE9pMIN"
   },
   "outputs": [],
   "source": [
    "# entrena neurona lineal y grafica\n",
    "modelo = NeuronaLineal(alpha=0.02, n_iter=50, cotaE=0.0001, draw=1, title=['Tama√±o motor', 'Precio'])\n",
    "modelo.fit(X, T)\n",
    "\n",
    "# imprime errores de cada iteracion:\n",
    "print('Errores:')\n",
    "for i, err in enumerate(modelo.errors_):\n",
    "    print('%3d => %f' % (i, err[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rELPdLScpPAJ",
   "metadata": {
    "id": "rELPdLScpPAJ"
   },
   "source": [
    "##### Prediccion de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q6KBcdXepRVm",
   "metadata": {
    "id": "q6KBcdXepRVm"
   },
   "outputs": [],
   "source": [
    "# invierte normalizaci√≥n para predecir\n",
    "tama√±o_motor = 130\n",
    "\n",
    "entrada = np.array([tama√±o_motor]).reshape((-1,1))\n",
    "\n",
    "if data_scaler:\n",
    "  entrada = data_scaler.transform(entrada)\n",
    "\n",
    "prediccion = modelo.predict( entrada )\n",
    "\n",
    "if targer_scaler:\n",
    "  prediccion = targer_scaler.inverse_transform([prediccion])\n",
    "\n",
    "# para la siguiente prueba tama√±o_motor debe coincidir con valores de engin-size en el data_frame\n",
    "\n",
    "autos_sel = df['engine-size']==tama√±o_motor\n",
    "valor_prom = df[autos_sel]['price'].mean()\n",
    "valor_min = df[autos_sel]['price'].min()\n",
    "valor_max = df[autos_sel]['price'].max()\n",
    "\n",
    "print('Tama√±o Motor.....:%.0f ' % tama√±o_motor)\n",
    "\n",
    "print('Prediccion Precio: %.2f\\n'% prediccion[0][0])\n",
    "\n",
    "print('  Precio Promedio: %.2f'% valor_prom)\n",
    "print('    Precio M√≠nimo: %.2f'% valor_min)\n",
    "print('    Precio M√°ximo: %.2f'% valor_max)\n",
    "\n",
    "display(df[autos_sel])# EJER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gEhn5fSipT7x",
   "metadata": {
    "id": "gEhn5fSipT7x"
   },
   "source": [
    "RESPUESTA: VERDADERO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hVRZyRMdqVmu",
   "metadata": {
    "id": "hVRZyRMdqVmu"
   },
   "source": [
    "‚úÖ **Verdadero.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç ¬øQu√© observamos?\n",
    "\n",
    "* **Con normalizaci√≥n**:\n",
    "\n",
    "  * El **ECM** (error cuadr√°tico medio) es razonable:\n",
    "\n",
    "    $$\n",
    "    \\text{ECM} \\approx 14,105,375\n",
    "    $$\n",
    "\n",
    "* **Sin normalizaci√≥n**:\n",
    "\n",
    "  * El **ECM explota**:\n",
    "\n",
    "    $$\n",
    "    \\text{ECM} \\approx 5.7 \\times 10^{43}\n",
    "    $$\n",
    "  * Los **coeficientes del modelo** son extremadamente grandes (del orden de $10^{14}$ a $10^{16}$), lo que indica inestabilidad num√©rica o divergencia.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è ¬øPor qu√© ocurre esto?\n",
    "\n",
    "El descenso por gradiente (SGD) es muy sensible a la **escala de los datos**. Si los atributos tienen rangos distintos (por ejemplo, `horsepower` en decenas y `curb-weight` en miles), los pasos de actualizaci√≥n del gradiente se vuelven desbalanceados, y el modelo puede divergir.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> **La afirmaci√≥n**\n",
    "> ‚ÄúEn este caso se comprob√≥ que si se omite la normalizaci√≥n el algoritmo de entrenamiento falla‚Äù\n",
    "> es **VERDADERA**.\n",
    "\n",
    "¬øQuer√©s que te muestre un gr√°fico comparando ambas predicciones?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FEHyJUwgqXBO",
   "metadata": {
    "id": "FEHyJUwgqXBO"
   },
   "source": [
    "# Ejercicio 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lvYhB3uRuH5l",
   "metadata": {
    "id": "lvYhB3uRuH5l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd      # para trabajar con archivos de datos csv, excel, etc: https://pandas.pydata.org/docs/getting_started/tutorials.html\n",
    "import chardet           # para detectar la codificaci√≥n de texto en archivos\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# importa neurona lineal de Laura en la carpeta Colab definida con FUENTES_DIR\n",
    "from ClassNeuronaLineal import NeuronaLineal\n",
    "\n",
    "nombre_archivo = DATOS_DIR + 'Autos.csv' # archivo de hojas\n",
    "\n",
    "#-- detectando la codificaci√≥n de caracteres usada ----\n",
    "with open(nombre_archivo, 'rb') as f:\n",
    "    result = chardet.detect(f.read())  # or readline if the file is large\n",
    "\n",
    "# recupera el archivo en un objeto dataframe de pandas utilizando la codificaci√≥n detectada\n",
    "df = pd.read_csv(nombre_archivo, encoding=result['encoding'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HeiRwCIOuI-n",
   "metadata": {
    "id": "HeiRwCIOuI-n"
   },
   "source": [
    "##### Preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XAatS-tXuPVu",
   "metadata": {
    "id": "XAatS-tXuPVu"
   },
   "outputs": [],
   "source": [
    "#-- seleccionar los atributos num√©ricos --\n",
    "df = df.select_dtypes(include = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"])\n",
    "\n",
    "# imprime valores nulos\n",
    "print(df.isnull().sum())\n",
    "\n",
    "nomAtrib = np.array(df.columns.values).reshape(-1,1)\n",
    "\n",
    "#-- ver si tienen datos faltantes --\n",
    "values = {'normalized-losses': df['normalized-losses'].mean(),\n",
    "          'bore': df['bore'].mean(),\n",
    "          'stroke': df['stroke'].mean(),\n",
    "          'horsepower': df['horsepower'].mean(),\n",
    "          'peak-rpm': df['peak-rpm'].mean(),\n",
    "          'price': df['price'].mean()}\n",
    "\n",
    "# reemplaza valores nulos por los indicados en el diccionario values\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "# Imprime matriz de correclacion\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(11,11))\n",
    "sns.heatmap(corr, square=True,  annot=True, linewidths=.5, cmap='coolwarm', annot_kws={'fontsize':9}, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1rouoEXuOb5",
   "metadata": {
    "id": "r1rouoEXuOb5"
   },
   "source": [
    "##### Preparaci√≥n de datos para entrenamiento de neurona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k6KsbFlUuT-C",
   "metadata": {
    "id": "k6KsbFlUuT-C"
   },
   "outputs": [],
   "source": [
    "Ejemplos = np.array(df)\n",
    "\n",
    "# separa aributos y target\n",
    "attPred = 14 #12 mpg y 14 precio  #-- nro de atributo a predecir\n",
    "T = Ejemplos[:, attPred].reshape(-1,1) # (N,) => (N,1)\n",
    "Ejemplos = np.delete(Ejemplos, [attPred], 1) #elimina la columna 12 a lo largo del eje 1\n",
    "\n",
    "nomPred = nomAtrib[attPred]\n",
    "nomCols = np.delete(nomAtrib, [attPred])\n",
    "\n",
    "alfa = 0.05\n",
    "MAX_ITE = 50\n",
    "CotaError = 10e-03\n",
    "CANT_PLOTS = 10\n",
    "\n",
    "normalizarEntrada= 1\n",
    "\n",
    "# normalizaci√≥n\n",
    "if normalizarEntrada == 1:\n",
    "    normalizador = preprocessing.MinMaxScaler()\n",
    "    X = normalizador.fit_transform(Ejemplos)\n",
    "elif normalizarEntrada == 2:\n",
    "    normalizador = preprocessing.StandardScaler()\n",
    "    X = normalizador.fit_transform(Ejemplos)\n",
    "else:\n",
    "    X = Ejemplos\n",
    "\n",
    "w_acc = np.zeros(len(nomCols)) # acumulador para pesos\n",
    "\n",
    "fig,subs=plt.subplots(1, CANT_PLOTS, sharex='col', sharey='row', figsize=(16,1.5))\n",
    "\n",
    "print('Normalizaci√≥n: %d' % normalizarEntrada)\n",
    "\n",
    "for i in range(0, CANT_PLOTS):\n",
    "    # Reordenar al azar X y T antes de cada entrenamiento\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices]\n",
    "    T = T[indices]\n",
    "\n",
    "    modelo = NeuronaLineal(alpha=alfa, n_iter=MAX_ITE, cotaE=CotaError)\n",
    "    modelo.fit(X, T)\n",
    "\n",
    "    w_acc += modelo.w_ # acumula pesos para despues promediar\n",
    "\n",
    "    # Colores para barras positivas y negativas. La idea es dibujarlas positivas\n",
    "    # para comparar la magnitud y pintar de otro color las negativas\n",
    "    colores = ['blue' if valor >= 0 else 'red' for valor in  modelo.w_]\n",
    "    subs[i].bar(np.arange(len(modelo.w_)), np.abs(modelo.w_), color=colores)\n",
    "    subs[i].set_xticks([])\n",
    "\n",
    "w_prom = w_acc / CANT_PLOTS\n",
    "\n",
    "# Color azul para barras positivas y rojo para negativas\n",
    "colores = ['blue' if valor >= 0 else 'red' for valor in w_prom]\n",
    "\n",
    "barras = np.abs(w_prom)\n",
    "plt.figure(figsize=(16,4))\n",
    "N=len(barras)\n",
    "\n",
    "plt.bar(np.arange(N), barras, color=colores)   # Gr√°fico de barras\n",
    "plt.title(nomPred)           # Colocamos el t√≠tulo\n",
    "plt.ylabel('W')\n",
    "plt.xticks(np.arange(N), nomCols, rotation='vertical')\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B3AsvpLduz73",
   "metadata": {
    "id": "B3AsvpLduz73"
   },
   "source": [
    "Aqu√≠ ten√©s los coeficientes promedio de los atributos num√©ricos tras 50 entrenamientos. Los atributos con **peso negativo** son aquellos que, al **incrementarse**, tienden a **disminuir el valor del precio** predicho.\n",
    "\n",
    "Seg√∫n los resultados:\n",
    "\n",
    "### ‚úÖ Atributos cuyo incremento **disminuye** el precio:\n",
    "\n",
    "* **city-mpg** ‚Üí incluido en tus opciones\n",
    "* **stroke**\n",
    "* **highway-mpg**\n",
    "* **bore**\n",
    "* **normalized-losses**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è Ahora evaluamos tus opciones:\n",
    "\n",
    "* a. **volume** ‚Üí no aparece (probablemente no es un atributo en este dataset) ‚ùå\n",
    "* b. **horsepower** ‚Üí tiene peso positivo, **no disminuye** el precio ‚ùå\n",
    "* c. **eco-rating** ‚Üí no est√° presente en este dataset ‚ùå\n",
    "* d. **engine-size** ‚Üí tiene peso positivo (no listado aqu√≠) ‚ùå\n",
    "* e. **city-mpg** ‚Üí **‚úÖ disminuye el precio**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "De tus opciones, **solo la opci√≥n e. city-mpg es correcta**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-5ozWQkmvIjh",
   "metadata": {
    "id": "-5ozWQkmvIjh"
   },
   "source": [
    "# Ejercicio 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PPVapWQivNET",
   "metadata": {
    "id": "PPVapWQivNET"
   },
   "source": [
    "‚ùå **Falso.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n:\n",
    "\n",
    "Una **neurona no lineal** con funci√≥n de activaci√≥n **sigmoide** tiene la forma:\n",
    "\n",
    "$$\n",
    "y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)\n",
    "\\quad \\text{donde} \\quad\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Aunque internamente realiza una **combinaci√≥n lineal** de las entradas, la salida es **no lineal** gracias a la funci√≥n sigmoide.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Diferencia clave:\n",
    "\n",
    "* Un **combinador lineal** (sin activaci√≥n no lineal) solo puede modelar relaciones **lineales**.\n",
    "* Una **neurona con activaci√≥n sigmoide** puede modelar relaciones **no lineales en la salida**, como por ejemplo:\n",
    "\n",
    "  * Clasificaci√≥n binaria (donde se requiere una salida entre 0 y 1)\n",
    "  * Separaci√≥n de clases linealmente separables en el espacio de caracter√≠sticas\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> **\"Una neurona no lineal con funci√≥n de transferencia sigmoide resuelve el mismo tipo de problemas que un combinador lineal\"**\n",
    "> es **FALSO**, porque **la neurona con sigmoide puede resolver m√°s casos** (como clasificaci√≥n binaria), mientras que el combinador lineal solo se limita a problemas lineales de regresi√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Tu93JofvYS6",
   "metadata": {
    "id": "2Tu93JofvYS6"
   },
   "source": [
    "# Ejercicio 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DIR_m34avaGv",
   "metadata": {
    "id": "DIR_m34avaGv"
   },
   "source": [
    "‚úÖ **Verdadero.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n:\n",
    "\n",
    "Un **perceptr√≥n** cl√°sico toma la forma:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } \\mathbf{w} \\cdot \\mathbf{x} + b \\geq 0 \\\\\n",
    "0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Es decir, utiliza una **funci√≥n de activaci√≥n escal√≥n** (step function) para producir una salida binaria.\n",
    "\n",
    "---\n",
    "\n",
    "Una **neurona no lineal con funci√≥n de activaci√≥n sigmoide**:\n",
    "\n",
    "$$\n",
    "y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)\n",
    "\\quad \\text{donde} \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "produce un **valor continuo entre 0 y 1**, pero se puede utilizar **para los mismos fines de clasificaci√≥n binaria**, interpretando su salida como una **probabilidad**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Comparaci√≥n:\n",
    "\n",
    "| Caracter√≠stica               | Perceptr√≥n              | Neurona con sigmoide       |\n",
    "| ---------------------------- | ----------------------- | -------------------------- |\n",
    "| Salida                       | 0 o 1                   | valor continuo en (0, 1)   |\n",
    "| Activaci√≥n                   | Escal√≥n (step)          | Sigmoide                   |\n",
    "| Capacidad para clasificaci√≥n | Solo problemas lineales | Tambi√©n lineales           |\n",
    "| Entrenamiento (derivable)    | No                      | S√≠ ‚úÖ (ideal para backprop) |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> **Una neurona no lineal con funci√≥n de transferencia sigmoide puede resolver el mismo tipo de problema que un perceptr√≥n.**\n",
    "\n",
    "Es **VERDADERO**: de hecho, es m√°s flexible y entrenable, especialmente con gradientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C72-3ScJvdyG",
   "metadata": {
    "id": "C72-3ScJvdyG"
   },
   "source": [
    "# Ejercicio 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625brAwvfS1",
   "metadata": {
    "id": "f625brAwvfS1"
   },
   "source": [
    "‚ùå **Falso.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n:\n",
    "\n",
    "Una **√∫nica neurona con funci√≥n de activaci√≥n sigmoide** \\*\\*solo puede resolver problemas de clasificaci√≥n binaria cuando las clases son **linealmente separables**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è ¬øPor qu√©?\n",
    "\n",
    "Porque la salida de la neurona est√° determinada por una combinaci√≥n lineal de las entradas:\n",
    "\n",
    "$$\n",
    "y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "Esto define una **frontera de decisi√≥n lineal** en el espacio de entrada. Por lo tanto:\n",
    "\n",
    "* Si las clases se pueden separar con una **recta (en 2D)**, un **plano (en 3D)**, etc., la neurona puede resolver el problema.\n",
    "* Si las clases **no son linealmente separables** (ej: problema XOR), una sola neurona **no alcanza**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ ¬øQu√© se necesita para casos no lineales?\n",
    "\n",
    "* Una **red neuronal con m√∫ltiples capas ocultas** (MLP) puede **aproximar funciones no lineales arbitrarias**.\n",
    "* Se requiere **composici√≥n de neuronas** para lograr **fronteras no lineales**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Ejemplo cl√°sico:\n",
    "\n",
    "* El problema **XOR** **no puede resolverse** con una sola neurona, aunque tenga activaci√≥n sigmoide.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> **‚ÄúPuede utilizarse una neurona no lineal con funci√≥n de activaci√≥n sigmoide para resolver cualquier problema de clasificaci√≥n de dos clases.‚Äù**\n",
    "> es **FALSO**, porque s√≥lo puede resolver problemas **linealmente separables**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qj6QAaA4vslM",
   "metadata": {
    "id": "Qj6QAaA4vslM"
   },
   "source": [
    "# Ejercicio 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Eg3qf39Iv26x",
   "metadata": {
    "id": "Eg3qf39Iv26x"
   },
   "source": [
    "‚úÖ **Verdadero (con matices).**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n t√©cnica:\n",
    "\n",
    "Tanto la funci√≥n **sigmoide** como la funci√≥n **tangente hiperb√≥lica (tanh)** son funciones de activaci√≥n **suaves y no lineales**, y ambas permiten que una neurona:\n",
    "\n",
    "* Modele **relaciones no lineales**.\n",
    "* Sea entrenada mediante **descenso del gradiente** (ambas son derivables).\n",
    "* Se use en tareas como **clasificaci√≥n binaria**, regresi√≥n no lineal, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Diferencias en pr√°ctica:\n",
    "\n",
    "| Caracter√≠stica  | Sigmoide $\\sigma(x)$               | Tanh $\\tanh(x)$                  |\n",
    "| --------------- | ---------------------------------- | -------------------------------- |\n",
    "| Rango de salida | (0, 1)                             | (‚àí1, 1)                          |\n",
    "| Simetr√≠a        | No (asim√©trica)                    | S√≠ (sim√©trica respecto a 0)      |\n",
    "| Uso com√∫n       | Salida de modelos de clasificaci√≥n | Capas ocultas en redes profundas |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è ¬øImporta la diferencia?\n",
    "\n",
    "* En teor√≠a, **una sola neurona con sigmoide o tanh puede resolver el mismo tipo de problemas** (por ejemplo, una separaci√≥n lineal).\n",
    "* En la pr√°ctica, **tanh puede acelerar el entrenamiento** en redes profundas por su simetr√≠a.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> **‚ÄúUna neurona no lineal con funci√≥n de transferencia sigmoide entre 0 y 1 resuelve el mismo tipo de problemas que una neurona no lineal con funci√≥n de transferencia tanh entre -1 y 1.‚Äù**\n",
    "> es **VERDADERO**, ya que ambas son funciones suaves y no lineales capaces de resolver el mismo tipo de problemas con una sola neurona.\n",
    "\n",
    "¬øQuer√©s ver esto comparado gr√°ficamente o con un ejemplo pr√°ctico en c√≥digo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ClJ_zqWqv3-g",
   "metadata": {
    "id": "ClJ_zqWqv3-g"
   },
   "source": [
    "# Ejercicio 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ESjDnfv54K",
   "metadata": {
    "id": "64ESjDnfv54K"
   },
   "source": [
    "‚ùå **Falso.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n:\n",
    "\n",
    "Un **combinador lineal** tiene la forma:\n",
    "\n",
    "$$\n",
    "y = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Para clasificaci√≥n binaria, suele usarse un **umbral**:\n",
    "\n",
    "$$\n",
    "\\text{clase} =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } y \\geq 0 \\\\\n",
    "0 & \\text{si } y < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Esto define una **frontera de decisi√≥n lineal** (una recta, plano, hiperplano).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitaci√≥n cr√≠tica:\n",
    "\n",
    "Un combinador lineal \\*\\*solo puede resolver problemas de clasificaci√≥n binaria si las clases son **linealmente separables**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Ejemplo:\n",
    "\n",
    "* ‚úÖ Puede resolver: distinguir entre dos clases separadas por una recta (ej. puntos arriba vs. abajo).\n",
    "* ‚ùå No puede resolver: el problema **XOR**, donde las clases no se pueden separar con una l√≠nea.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusi√≥n:\n",
    "\n",
    "> **‚ÄúPuede utilizarse un combinador lineal para resolver cualquier problema de clasificaci√≥n de dos clases.‚Äù**\n",
    "> es **FALSO**, porque solo funciona cuando las clases son **linealmente separables**. Para casos no lineales, se necesita una red m√°s compleja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JK8um1GewCxJ",
   "metadata": {
    "id": "JK8um1GewCxJ"
   },
   "source": [
    "# Ejercicio 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ah-R4cmHwEXY",
   "metadata": {
    "id": "ah-R4cmHwEXY"
   },
   "source": [
    "‚úÖ **Verdadero.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explicaci√≥n:\n",
    "\n",
    "La **funci√≥n softmax** se usa generalmente para clasificaci√≥n **multiclase**, y tiene la forma:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Pero cuando se aplica sobre una **sola neurona** (es decir, un √∫nico valor $z$), se cumple:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z) = \\frac{e^z}{e^z} = 1\n",
    "$$\n",
    "\n",
    "Entonces **no tiene sentido usar softmax con una sola neurona** de salida de manera directa.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Lo que s√≠ es v√°lido:\n",
    "\n",
    "En **clasificaci√≥n binaria**, se suelen usar **dos neuronas en la capa de salida con softmax** (una por clase). Esto s√≠ aplica correctamente:\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{softmax}([z_1, z_2]) =\n",
    "\\left[ \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}, \\frac{e^{z_2}}{e^{z_1} + e^{z_2}} \\right]\n",
    "$$\n",
    "\n",
    "Alternativamente, **se puede usar una sola neurona con activaci√≥n sigmoide**, que es funcionalmente equivalente a usar softmax sobre 2 clases.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Conclusi√≥n final:\n",
    "\n",
    "> **‚ÄúPuede utilizarse un modelo con una capa de salida softmax formada por una sola neurona para resolver un problema de clasificaci√≥n binaria.‚Äù**\n",
    "\n",
    "es **‚úÖ VERDADERO** **si interpretamos que hay una segunda clase impl√≠cita** (es decir, que se usa softmax para producir una probabilidad sobre dos clases, y solo se retorna una de ellas). Pero en la pr√°ctica se prefiere:\n",
    "\n",
    "* **Softmax con 2 neuronas** para binaria multiclase expl√≠cita.\n",
    "* **Sigmoide con 1 neurona** para binaria est√°ndar.\n",
    "\n",
    "---\n",
    "\n",
    "¬øQuer√©s que te muestre esto con un ejemplo num√©rico o gr√°fico?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U31KWU8jwVrP",
   "metadata": {
    "id": "U31KWU8jwVrP"
   },
   "source": [
    "# Ejercicio 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fjrg4HyV0iHD",
   "metadata": {
    "id": "Fjrg4HyV0iHD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd      # para trabajar con archivos de datos csv, excel, etc: https://pandas.pydata.org/docs/getting_started/tutorials.html\n",
    "import chardet           # para detectar la codificaci√≥n de texto en archivos\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ClassRNMulticlase import RNMulticlase\n",
    "\n",
    "nombre_archivo = DATOS_DIR + 'Vinos.csv' # archivo de hojas\n",
    "\n",
    "#-- detectando la codificaci√≥n de caracteres usada ----\n",
    "with open(nombre_archivo, 'rb') as f:\n",
    "    result = chardet.detect(f.read())  # or readline if the file is large\n",
    "\n",
    "# recupera el archivo en un objeto dataframe de pandas utilizando la codificaci√≥n detectada\n",
    "df = pd.read_csv(nombre_archivo, encoding=result['encoding'], sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3EZSNEf20lUw",
   "metadata": {
    "id": "3EZSNEf20lUw"
   },
   "source": [
    "##### Selecci√≥n de atributos y normalizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2QcLjraV0pW2",
   "metadata": {
    "id": "2QcLjraV0pW2"
   },
   "outputs": [],
   "source": [
    "# %% separa atributos para entrenar de clases\n",
    "X_raw = np.array(df.iloc[:,1::])  # recupera todas las columnas salvo la primera (es la clase)\n",
    "T_raw = np.array(df.iloc[:,0])    # recupera solo la primera columna (es la clase)\n",
    "\n",
    "clases = np.unique(T_raw)  # obtiene las clases sin repeticiones\n",
    "print('Las clases del dataset son :', clases)\n",
    "\n",
    "# Normalizacion con media y desviacion\n",
    "scaler = preprocessing.StandardScaler()\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "X = scaler.fit_transform(X_raw)\n",
    "\n",
    "T_raw = T_raw.reshape((-1,1))   # fuerza formato (N,1) sino es (N,)\n",
    "\n",
    "# los target deben tener formato one hot.\n",
    "# clase 1 => 100 | clase 2 => 010 | clase 3 => 001\n",
    "encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "T = encoder.fit_transform(T_raw.reshape(-1,1)).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QC6YOOuT0rno",
   "metadata": {
    "id": "QC6YOOuT0rno"
   },
   "source": [
    "##### Entrenamiento de neurona multiclase con activaci√≥n \"tanh\" y error \"EC_binaria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y4Vn9LXO0uk5",
   "metadata": {
    "id": "y4Vn9LXO0uk5"
   },
   "outputs": [],
   "source": [
    "# Texto para acumular los resultados en formato de tabla\n",
    "# Se imprime al final de cada experimento con los resultados acumulados\n",
    "encabezado = \"|{:^7}|{:^5}|{:^8}|{:^7}|{:^7}|{:^10}|{:6}|{:6}|{:^8}|{:^8}|\".format(\n",
    "    \"div E/P\", \"ALFA\", \"FUN\", \"MAX_ITE\", \"ERROR\", \"COSTO\",\n",
    "    \"Entren\", \"Prueba\", \"100% Ent\", \"100% Prb\"\n",
    ")\n",
    "separador = \"|{:-^7}|{:-^5}|{:-^8}|{:-^7}|{:-^7}|{:-^10}|{:-^6}|{:-^6}|{:-^8}|{:-^8}|\".format(\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    ")\n",
    "\n",
    "tabla_resultados = encabezado + \"\\n\" + separador + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "izd7EbDB0xP-",
   "metadata": {
    "id": "izd7EbDB0xP-"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# neurona multiclase con sigmoid y ECM\n",
    "ALFA = 0.2       # tasa de aprendizaje\n",
    "#FUN = 'sigmoid'  # funciones de activacion\n",
    "FUN = 'tanh'\n",
    "#FUN = 'softmax'\n",
    "MAX_ITE = 400    # cant maxima de iteraciones\n",
    "ERROR = 0.00001    # cota para considerar entrenado el modelo\n",
    "CORRIDAS = 30    # veces para repetir el experimento\n",
    "TEST_SIZE = .4   # para division en entrenamiento y prueba\n",
    "COSTO = 'EC_binaria'    # funciones de costo\n",
    "#COSTO = 'ECM' | 'EC' | 'EC_binaria'\n",
    "\n",
    "print('\\nNeurona con %s\\n' %FUN)\n",
    "\n",
    "prom_ite = 0\n",
    "prom_acc_train = 0\n",
    "prom_acc_test = 0\n",
    "veces_train = 0\n",
    "veces_test = 0\n",
    "\n",
    "for nro_ite in range(CORRIDAS):\n",
    "    # Establece resultado esperado seg√∫n la clase. 1=clase esperada, 0=otra clase\n",
    "\n",
    "    #Importante: la salida de la funci√≥n de activaci√≥n debe estar en la misma escala que la clase a predecir\n",
    "    if FUN == 'tanh':\n",
    "        T_new = 2*T-1\n",
    "    else:\n",
    "        T_new = T\n",
    "\n",
    "    # genera diferentes divisiones del dataset en cada iteraci√≥n\n",
    "    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, T_new, test_size=TEST_SIZE, random_state=nro_ite)\n",
    "\n",
    "    # Entrena modelo\n",
    "    modelo = RNMulticlase(alpha=ALFA, n_iter=MAX_ITE, FUN=FUN, COSTO=COSTO, cotaE=ERROR)\n",
    "    modelo.fit(X_train, Y_train)\n",
    "\n",
    "    (W, b, iteraciones) = (modelo.w_, modelo.b_, len(modelo.errors_))\n",
    "\n",
    "    prom_ite = prom_ite + iteraciones\n",
    "\n",
    "    # efectividad entrenamiento\n",
    "    efectividad = 100*modelo.accuracy(X_train, Y_train)\n",
    "\n",
    "    prom_acc_train = prom_acc_train + efectividad\n",
    "    if (100-efectividad < 0.00001):\n",
    "      veces_train = veces_train + 1\n",
    "\n",
    "    print(\"Entrenamiento: %6.2f%% de efectividad con %2d iteraciones\" % (efectividad, iteraciones ))\n",
    "    # efectividad testeo\n",
    "    efectividad = 100*modelo.accuracy(X_test, Y_test)\n",
    "\n",
    "    prom_acc_test = prom_acc_test + efectividad\n",
    "    if (100-efectividad < 0.00001):\n",
    "      veces_test = veces_test + 1\n",
    "\n",
    "    print(\"Prueba.......: %6.2f%% de efectividad con %2d iteraciones\" % (efectividad, iteraciones ))\n",
    "\n",
    "print('-'*80)\n",
    "print('promedio iteraciones  : %6.2f' % (prom_ite/CORRIDAS))\n",
    "print('promedio acc. train   : %6.2f' % (prom_acc_train/CORRIDAS))\n",
    "print('promedio acc. test    : %6.2f' % (prom_acc_test/CORRIDAS))\n",
    "print('ejec. train con 100%%  : %3d' % (veces_train))\n",
    "print('ejec.  test con 100%%  : %3d' % (veces_test))\n",
    "\n",
    "\n",
    "# A√±adir fila a la tabla con par√°metros y resultados\n",
    "fila = \"|{:^7}|{:^5.1f}|{:^8}|{:^7}|{:^7.0e}|{:^10}|{:6.2f}|{:6.2f}|{:^8}|{:^8}|\".format(\n",
    "    TEST_SIZE,\n",
    "    ALFA,\n",
    "    FUN,\n",
    "    MAX_ITE,\n",
    "    ERROR,\n",
    "    COSTO,\n",
    "    prom_acc_train//CORRIDAS,\n",
    "    prom_acc_test//CORRIDAS,\n",
    "    veces_train,\n",
    "    veces_test\n",
    ")\n",
    "tabla_resultados += fila + \"\\n\"\n",
    "\n",
    "# Imprimir tabla completa\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RESULTADOS DETALLADOS DE TODAS LAS CORRIDAS:\\n\")\n",
    "print(tabla_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Eia3xB2o1hdz",
   "metadata": {
    "id": "Eia3xB2o1hdz"
   },
   "source": [
    "El archivo **`Vinos.csv`** tiene todos los datos en una √∫nica columna con separador `;`, por lo que vamos a cargarlo correctamente primero. Luego, evaluamos la afirmaci√≥n sobre usar una **capa de salida con 3 neuronas tanh + entrop√≠a cruzada binaria**.\n",
    "\n",
    "\n",
    "El dataset contiene:\n",
    "\n",
    "* **14 columnas**, de las cuales la variable **`Class`** es la clase a predecir.\n",
    "* **3 clases diferentes**: `[1, 2, 3]` ‚Üí Es un problema de **clasificaci√≥n multiclase (3 clases).**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Afirmaci√≥n:\n",
    "\n",
    "> ‚ÄúEs posible clasificar las muestras con una red que tenga una **capa de salida formada por 3 neuronas con activaci√≥n tanh** y **funci√≥n de costo entrop√≠a cruzada binaria**.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **FALSO.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Justificaci√≥n:\n",
    "\n",
    "* **`tanh`** produce salidas en el rango **(-1, 1)**, pero:\n",
    "\n",
    "  * La **entrop√≠a cruzada binaria** espera salidas en el rango **(0, 1)** (como las que da una sigmoide o softmax).\n",
    "  * Adem√°s, **entrop√≠a cruzada binaria** se usa para **clasificaci√≥n binaria o multilabel**, no para multiclase con softmax-style outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Lo correcto ser√≠a:\n",
    "\n",
    "Para este problema de **clasificaci√≥n multiclase con 3 clases**:\n",
    "\n",
    "* **Capa de salida**: 3 neuronas con **activaci√≥n softmax**.\n",
    "* **Funci√≥n de p√©rdida**: **categorical crossentropy** (o sparse categorical crossentropy si las clases son enteros).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Conclusi√≥n:\n",
    "\n",
    "> ‚ÄúEs posible clasificar las muestras con una red con 3 neuronas `tanh` y p√©rdida `binary crossentropy`.‚Äù\n",
    "\n",
    "Es **FALSO**, porque:\n",
    "\n",
    "* El **rango de activaci√≥n y la funci√≥n de p√©rdida no son compatibles**.\n",
    "* El problema es multiclase, no multilabel ni binario.\n",
    "\n",
    "¬øQuer√©s que te d√© el c√≥digo correcto en Keras para este caso?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ehd5CkbS2qeu",
   "metadata": {
    "id": "ehd5CkbS2qeu"
   },
   "source": [
    "# Ejercicio 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q9TbnZ8z2sJ_",
   "metadata": {
    "id": "q9TbnZ8z2sJ_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import tanh\n",
    "\n",
    "# Cargar el archivo\n",
    "df = pd.read_csv(\"FrutasTrain.csv\")  # Asegurate de ajustar el path si es necesario\n",
    "\n",
    "# Extraer caracter√≠sticas\n",
    "X = df[['Diametro', 'Color']].values\n",
    "\n",
    "# Normalizar con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Pesos dados de la neurona entrenada\n",
    "W = np.array([2.40, -2.39])\n",
    "b = -0.065\n",
    "\n",
    "# Calcular z = W¬∑X + b\n",
    "z = X_scaled @ W + b\n",
    "\n",
    "# Aplicar activaci√≥n tanh\n",
    "salidas = tanh(z)\n",
    "\n",
    "# Clasificaci√≥n:\n",
    "# - Si salida ‚â• 0.8 ‚Üí clase 1\n",
    "# - Si salida ‚â§ -0.8 ‚Üí clase -1\n",
    "# - Si salida ‚àà (-0.8, 0.8) ‚Üí indefinido\n",
    "indefinidos = np.sum((salidas > -0.8) & (salidas < 0.8))\n",
    "\n",
    "print(\"Cantidad de ejemplos indefinidos:\", indefinidos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UoYp3lgj20ns",
   "metadata": {
    "id": "UoYp3lgj20ns"
   },
   "source": [
    "‚úÖ La cantidad de ejemplos de entrenamiento clasificados como **‚Äúindefinido‚Äù** es:\n",
    "\n",
    "**üî¢ 5 ejemplos**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Seg√∫n el criterio:\n",
    "\n",
    "* Salida ‚â• 0.8 ‚Üí clase **1**\n",
    "* Salida ‚â§ -0.8 ‚Üí clase **‚àí1**\n",
    "* Salida ‚àà (‚àí0.8, 0.8) ‚Üí **indefinido**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Opci√≥n correcta:\n",
    "\n",
    "**f. 5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AGK4rZxH3YFt",
   "metadata": {
    "id": "AGK4rZxH3YFt"
   },
   "source": [
    "# Ejercicio 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VqbxCARP3ae0",
   "metadata": {
    "id": "VqbxCARP3ae0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import tanh\n",
    "\n",
    "# Cargar los datos de entrenamiento\n",
    "df = pd.read_csv(\"FrutasTrain.csv\")  # Asegurarse que el archivo est√© en el path correcto\n",
    "\n",
    "# Extraer caracter√≠sticas num√©ricas\n",
    "X = df[['Diametro', 'Color']].values\n",
    "\n",
    "# Normalizar los datos de entrenamiento\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Definir los pesos de la neurona entrenada\n",
    "W = np.array([2.40, -2.39])\n",
    "b = -0.065\n",
    "\n",
    "# Nueva fruta: di√°metro = 14, color = 140\n",
    "nueva_fruta = np.array([[14, 140]])\n",
    "\n",
    "# Aplicar la misma normalizaci√≥n\n",
    "nueva_fruta_scaled = scaler.transform(nueva_fruta)\n",
    "\n",
    "# Calcular salida de la neurona\n",
    "z_nueva = nueva_fruta_scaled @ W + b\n",
    "salida_nueva = tanh(z_nueva[0])\n",
    "\n",
    "# Clasificar seg√∫n el criterio\n",
    "if salida_nueva >= 0.8:\n",
    "    resultado = \"Naranja\"\n",
    "elif salida_nueva <= -0.8:\n",
    "    resultado = \"Mel√≥n\"\n",
    "else:\n",
    "    resultado = \"Indefinido\"\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"Salida de la neurona: {salida_nueva:.4f}\")\n",
    "print(f\"Clasificaci√≥n: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zJ3FswGq3bmE",
   "metadata": {
    "id": "zJ3FswGq3bmE"
   },
   "source": [
    "‚úÖ La salida de la neurona para una fruta con:\n",
    "\n",
    "* **Di√°metro = 14 cm**\n",
    "* **Color = 140**\n",
    "\n",
    "es aproximadamente **‚àí0.97**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Clasificaci√≥n:\n",
    "\n",
    "* Como la salida es **menor o igual que ‚àí0.8**, se clasifica como:\n",
    "\n",
    "**ü•≠ Mel√≥n**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Opci√≥n correcta:\n",
    "\n",
    "**b. Mel√≥n**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3asRc3p3v",
   "metadata": {
    "id": "b7e3asRc3p3v"
   },
   "source": [
    "# Ejercicio 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KvnW06hU3rOY",
   "metadata": {
    "id": "KvnW06hU3rOY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import tanh\n",
    "\n",
    "# Cargar datos de entrenamiento\n",
    "df = pd.read_csv(\"FrutasTrain.csv\")  # Asegurate de que el archivo est√© en el mismo directorio\n",
    "\n",
    "# Extraer caracter√≠sticas\n",
    "X = df[['Diametro', 'Color']].values\n",
    "\n",
    "# Normalizar los datos con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Definir pesos de la neurona entrenada\n",
    "W = np.array([2.40, -2.39])\n",
    "b = -0.065\n",
    "\n",
    "# Nueva fruta a clasificar: di√°metro = 16, color = 79\n",
    "nueva_fruta = np.array([[16, 79]])\n",
    "\n",
    "# Normalizar la nueva muestra con el mismo scaler\n",
    "nueva_fruta_scaled = scaler.transform(nueva_fruta)\n",
    "\n",
    "# Calcular la salida de la neurona\n",
    "z = nueva_fruta_scaled @ W + b\n",
    "salida = tanh(z[0])\n",
    "\n",
    "# Clasificar seg√∫n criterio definido\n",
    "if salida >= 0.8:\n",
    "    resultado = \"Naranja\"\n",
    "elif salida <= -0.8:\n",
    "    resultado = \"Mel√≥n\"\n",
    "else:\n",
    "    resultado = \"Indefinido\"\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Salida de la neurona: {salida:.4f}\")\n",
    "print(f\"Clasificaci√≥n: {resultado}\")\n",
    "\n",
    "# Salida de la neurona: 0.5918\n",
    "# Clasificaci√≥n: Indefinido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GLNM1R0Z3ubh",
   "metadata": {
    "id": "GLNM1R0Z3ubh"
   },
   "source": [
    "‚úÖ La salida de la neurona para una fruta con:\n",
    "\n",
    "* **Di√°metro = 16 cm**\n",
    "* **Color = 79**\n",
    "\n",
    "es aproximadamente **0.59**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Clasificaci√≥n:\n",
    "\n",
    "Como la salida est√° en el rango **(‚àí0.8, 0.8)**, la clasificaci√≥n es:\n",
    "\n",
    "**üö´ Indefinido**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Opci√≥n correcta:\n",
    "\n",
    "**b. Indefinido**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "examen02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
