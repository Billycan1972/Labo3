{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d65aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbase=pd.read_csv(\"dataset_base.csv\")\n",
    "dfbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbasefeatures=pd.read_csv(\"dataset_base_features.csv\")\n",
    "dfbasefeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744be25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"dataset_base.csv\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'])\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Listado de productos\n",
    "productos = df['product_id'].unique()\n",
    "\n",
    "# 3. Salida\n",
    "resultados = []\n",
    "log = []\n",
    "\n",
    "# 4. Directorios temporales\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['year'] = datos['periodo'].dt.year\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-12-01'].copy()\n",
    "    val = datos[datos['periodo'] == '2019-12-01'].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Regresion lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=1)\n",
    "        maes['arima'] = mean_absolute_error(y_val, y_pred)\n",
    "        feb_pred = modelo_arima.forecast(steps=3)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon TimeSeries\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie,\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=2,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\n",
    "                \"ETS\": {},\n",
    "                \"AutoARIMA\": {},\n",
    "                \"Naive\": {},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        pred_feb = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, [forecast.loc[(str(prod), pd.Timestamp(\"2019-12-01\")), 'mean']])\n",
    "        preds['autogluon'] = pred_feb\n",
    "\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    # Elegir el mejor modelo\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    linea_log = f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE dic-2019 = {maes[mejor_modelo]:.4f}\"\n",
    "    print(linea_log)\n",
    "    log.append(linea_log)\n",
    "\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "\n",
    "# 6. Exportar\n",
    "pd.DataFrame(resultados).to_csv(\"predicciones_febrero2020_porproducto.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "print(\"\\n Prediccion final guardada en 'predicciones_febrero2020_porproducto.csv'\")\n",
    "print(\"ðŸ“„ Log guardado en 'log_modelos.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"dataset_base_features.csv\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'])\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Listado de productos\n",
    "productos = df['product_id'].unique()\n",
    "\n",
    "# 3. Salida\n",
    "resultados = []\n",
    "log = []\n",
    "\n",
    "# 4. Directorios temporales\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['year'] = datos['periodo'].dt.year\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-12-01'].copy()\n",
    "    val = datos[datos['periodo'] == '2019-12-01'].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Regresion lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=1)\n",
    "        maes['arima'] = mean_absolute_error(y_val, y_pred)\n",
    "        feb_pred = modelo_arima.forecast(steps=3)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=722,\n",
    "            learning_rate=0.26830103566346203,\n",
    "            max_depth=8,\n",
    "            num_leaves=302,\n",
    "            min_data_in_leaf=65,\n",
    "            min_child_weight=0.2723737879682162,\n",
    "            subsample=0.6693558818396728,\n",
    "            subsample_freq=3,\n",
    "            colsample_bytree=0.9982366151830648,\n",
    "            colsample_bynode=0.6808843764592971,\n",
    "            reg_alpha=1.5889509640833777,\n",
    "            reg_lambda=3.277536337616617,\n",
    "            max_bin=502,\n",
    "            min_split_gain=0.10250744462326401,\n",
    "            cat_smooth=49.797959349843936,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon TimeSeries\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie,\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=2,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\n",
    "                \"ETS\": {},\n",
    "                \"AutoARIMA\": {},\n",
    "                \"Naive\": {},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        pred_feb = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, [forecast.loc[(str(prod), pd.Timestamp(\"2019-12-01\")), 'mean']])\n",
    "        preds['autogluon'] = pred_feb\n",
    "\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    # Elegir el mejor modelo\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    linea_log = f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE dic-2019 = {maes[mejor_modelo]:.4f}\"\n",
    "    print(linea_log)\n",
    "    log.append(linea_log)\n",
    "\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "\n",
    "# 6. Exportar\n",
    "pd.DataFrame(resultados).to_csv(\"predicciones_febrero2020_porproducto1.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos1.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "print(\"\\n Prediccion final guardada en 'predicciones_febrero2020_porproducto1.csv'\")\n",
    "print(\"ðŸ“„ Log guardado en 'log_modelos1.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset sell-in\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "\n",
    "# 4. Crear carpeta para modelos AutoGluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['year'] = datos['periodo'].dt.year\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-12-01'].copy()\n",
    "    val = datos[datos['periodo'] == '2019-12-01'].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Regresion lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=1)\n",
    "        maes['arima'] = mean_absolute_error(y_val, y_pred)\n",
    "        feb_pred = modelo_arima.forecast(steps=3)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon TimeSeries\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie,\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=2,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\n",
    "                \"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}\n",
    "            }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        pred_feb = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, [forecast.loc[(str(prod), pd.Timestamp(\"2019-12-01\")), 'mean']])\n",
    "        preds['autogluon'] = pred_feb\n",
    "\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    # Elegir el mejor modelo\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    linea_log = f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE dic-2019 = {maes[mejor_modelo]:.4f}\"\n",
    "    print(linea_log)\n",
    "    log.append(linea_log)\n",
    "\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "\n",
    "# 6. Exportar\n",
    "pd.DataFrame(resultados).to_csv(\"predicciones_febrero2020_porproducto2.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos2.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ… PredicciÃ³n final guardada en 'predicciones_febrero2020_porproducto2.csv'\")\n",
    "print(\"ðŸ“„ Log guardado en 'log_modelos2.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fafdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "\n",
    "# 4. Crear carpeta para modelos AutoGluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['year'] = datos['periodo'].dt.year\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-12-01'].copy()\n",
    "    val = datos[datos['periodo'] == '2019-12-01'].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Regresion lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=1)\n",
    "        maes['arima'] = mean_absolute_error(y_val, y_pred)\n",
    "        feb_pred = modelo_arima.forecast(steps=3)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon TimeSeries\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie,\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=2,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\n",
    "                \"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}\n",
    "            }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        pred_feb = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, [forecast.loc[(str(prod), pd.Timestamp(\"2019-12-01\")), 'mean']])\n",
    "        preds['autogluon'] = pred_feb\n",
    "\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    # Elegir el mejor modelo\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE dic-2019 = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "# 6. Fallback: completar productos faltantes con promedio Ãºltimos 12 meses\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    if ultimos_12.empty:\n",
    "        pred_fallback = 0\n",
    "    else:\n",
    "        pred_fallback = ultimos_12['tn'].mean()\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Exportar\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "resultados_df = resultados_df.sort_values(\"product_id\")\n",
    "resultados_df.to_csv(\"predicciones_febrero2020_porproducto2.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos2.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ… PredicciÃ³n final guardada en 'predicciones_febrero2020_porproducto2.csv'\")\n",
    "print(\"ðŸ“„ Log guardado en 'log_modelos2.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blend\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "\n",
    "# 4. Crear carpeta para modelos AutoGluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['year'] = datos['periodo'].dt.year\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-12-01'].copy()\n",
    "    val = datos[datos['periodo'] == '2019-12-01'].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Regresion lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "        preds['regresion'] = 0\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=1)\n",
    "        maes['arima'] = mean_absolute_error(y_val, y_pred)\n",
    "        feb_pred = modelo_arima.forecast(steps=3)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "        preds['arima'] = 0\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "        preds['lgbm'] = 0\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "        preds['xgboost'] = 0\n",
    "\n",
    "    # 5. AutoGluon TimeSeries\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie,\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=2,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=120,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={ \"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {} }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        pred_feb = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, [forecast.loc[(str(prod), pd.Timestamp(\"2019-12-01\")), 'mean']])\n",
    "        preds['autogluon'] = pred_feb\n",
    "\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "        preds['autogluon'] = 0\n",
    "\n",
    "    # PredicciÃ³n final blended\n",
    "    pred_final = (\n",
    "        0.30 * preds['lgbm'] +\n",
    "        0.25 * preds['regresion'] +\n",
    "        0.25 * preds['arima'] +\n",
    "        0.15 * preds['autogluon'] +\n",
    "        0.10 * preds['xgboost']\n",
    "    )\n",
    "\n",
    "    productos_predichos.add(prod)\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    linea_log = (\n",
    "        f\"Producto {prod}: MAEs - LGBM={maes['lgbm']:.4f}, Reg={maes['regresion']:.4f}, \"\n",
    "        f\"ARIMA={maes['arima']:.4f}, AutoGluon={maes['autogluon']:.4f}, XGB={maes['xgboost']:.4f}\"\n",
    "    )\n",
    "    log.append(linea_log)\n",
    "\n",
    "# 6. Fallback: completar productos faltantes con promedio Ãºltimos 12 meses\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Exportar\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "resultados_df = resultados_df.sort_values(\"product_id\")\n",
    "resultados_df.to_csv(\"predicciones_febrero2020_blended.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos_blended.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ… PredicciÃ³n final guardada en 'predicciones_febrero2020_blended.csv'\")\n",
    "print(\"ðŸ“„ Log guardado en 'log_modelos_blended.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fe979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blend 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "\n",
    "# 4. Crear carpeta para modelos AutoGluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['year'] = datos['periodo'].dt.year\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-12-01'].copy()\n",
    "    val = datos[datos['periodo'] == '2019-12-01'].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Regresion lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "        preds['regresion'] = 0\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=1)\n",
    "        maes['arima'] = mean_absolute_error(y_val, y_pred)\n",
    "        feb_pred = modelo_arima.forecast(steps=3)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "        preds['arima'] = 0\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "        preds['lgbm'] = 0\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "        preds['xgboost'] = 0\n",
    "\n",
    "    # 5. AutoGluon TimeSeries\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie,\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=2,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=120,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={ \"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {} }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        pred_feb = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, [forecast.loc[(str(prod), pd.Timestamp(\"2019-12-01\")), 'mean']])\n",
    "        preds['autogluon'] = pred_feb\n",
    "\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "        preds['autogluon'] = 0\n",
    "\n",
    "    # PredicciÃ³n final blended\n",
    "    pred_final = (\n",
    "        0.45 * preds['lgbm'] +\n",
    "        0.25 * preds['regresion'] +\n",
    "        0.05 * preds['arima'] +\n",
    "        0.15 * preds['autogluon'] +\n",
    "        0.10 * preds['xgboost']\n",
    "    )\n",
    "\n",
    "    productos_predichos.add(prod)\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    linea_log = (\n",
    "        f\"Producto {prod}: MAEs - LGBM={maes['lgbm']:.4f}, Reg={maes['regresion']:.4f}, \"\n",
    "        f\"ARIMA={maes['arima']:.4f}, AutoGluon={maes['autogluon']:.4f}, XGB={maes['xgboost']:.4f}\"\n",
    "    )\n",
    "    log.append(linea_log)\n",
    "\n",
    "# 6. Fallback: completar productos faltantes con promedio Ãºltimos 12 meses\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Exportar\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "resultados_df = resultados_df.sort_values(\"product_id\")\n",
    "resultados_df.to_csv(\"predicciones_febrero2020_blended1.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos_blended1.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "print(\"\\nâœ… PredicciÃ³n final guardada en 'predicciones_febrero2020_blended1.csv'\")\n",
    "print(\"ðŸ“„ Log guardado en 'log_modelos_blended1.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c074e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validaciÃ³n ampliada a septiembre, octubre y noviembre. 0.260 en el public\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "\n",
    "# 4. Carpeta autogluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto3.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo.csv\", index=False)\n",
    "with open(\"log_modelos3.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# 8. GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a784409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validaciÃ³n 2019\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "# 4. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'].dt.year <= 2018]\n",
    "    val = datos[datos['periodo'].dt.year == 2019]\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=12)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=14)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=14,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(f\"2019-{m:02d}-01\")), 'mean'] for m in range(1, 13)]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE 2019 = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar salidas\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto4.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo4.csv\", index=False)\n",
    "with open(\"log_modelos4.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# 8. GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (ValidaciÃ³n 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validaciÃ³n ampliada a septiembre, octubre, noviembre y diciembre. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "\n",
    "# 4. Carpeta autogluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01'),\n",
    "        pd.Timestamp('2019-12-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto6.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo6.csv\", index=False)\n",
    "with open(\"log_modelos6.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# 8. GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ff0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HORRIBLE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar logging para errores y mensajes\n",
    "logging.basicConfig(filename='log_modelos7.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "maes_resumen = []\n",
    "productos_predichos = set()\n",
    "\n",
    "# 4. Carpeta autogluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "# FunciÃ³n para crear features temporales adicionales\n",
    "def crear_features_temporales(df):\n",
    "    df = df.copy()\n",
    "    df['mes'] = df['periodo'].dt.month\n",
    "    df['aÃ±o'] = df['periodo'].dt.year\n",
    "    df['tendencia'] = np.arange(len(df))  # Ã­ndice temporal para tendencia lineal\n",
    "    meses_dummies = pd.get_dummies(df['mes'], prefix='mes', drop_first=True)\n",
    "    df = pd.concat([df, meses_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos = crear_features_temporales(datos)\n",
    "\n",
    "    # Entrenamiento: hasta agosto 2019 inclusive\n",
    "    train = datos[datos['periodo'] <= '2019-08-01'].copy()\n",
    "    # ValidaciÃ³n: septiembre, octubre y noviembre 2019\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        logging.warning(f\"Producto {prod}: datos insuficientes para entrenamiento o validaciÃ³n.\")\n",
    "        continue\n",
    "\n",
    "    feature_cols = ['mes', 'tendencia'] + [col for col in datos.columns if col.startswith('mes_')]\n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[feature_cols]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        tendencia_feb = train['tendencia'].max() + 6\n",
    "        mes_feb = 2\n",
    "        meses_feb = [0]*(len(feature_cols)-2)\n",
    "        for i, col in enumerate(feature_cols[2:]):\n",
    "            if col == 'mes_2':\n",
    "                meses_feb[i] = 1\n",
    "        X_feb = [mes_feb, tendencia_feb] + meses_feb\n",
    "        preds['regresion'] = lr.predict([X_feb])[0]\n",
    "    except Exception as e:\n",
    "        maes['regresion'] = np.inf\n",
    "        logging.error(f\"Producto {prod} - RegresiÃ³n lineal: {e}\")\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=len(val))\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        preds['arima'] = modelo_arima.forecast(steps=6)[-1]\n",
    "    except Exception as e:\n",
    "        maes['arima'] = np.inf\n",
    "        logging.error(f\"Producto {prod} - ARIMA: {e}\")\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.0645,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.79,\n",
    "            subsample=0.703,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.989,\n",
    "            colsample_bynode=0.815,\n",
    "            reg_alpha=4.96,\n",
    "            reg_lambda=3.82,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.0063,\n",
    "            cat_smooth=49.8,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([X_feb])[0]\n",
    "    except Exception as e:\n",
    "        maes['lgbm'] = np.inf\n",
    "        logging.error(f\"Producto {prod} - LightGBM: {e}\")\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0, random_state=42)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([X_feb])[0]\n",
    "    except Exception as e:\n",
    "        maes['xgboost'] = np.inf\n",
    "        logging.error(f\"Producto {prod} - XGBoost: {e}\")\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}_7\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=300,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except Exception as e:\n",
    "        maes['autogluon'] = np.inf\n",
    "        logging.error(f\"Producto {prod} - AutoGluon: {e}\")\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "\n",
    "    logging.info(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback para productos sin predicciÃ³n\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    logging.warning(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar resultados y MAE\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto7.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo7.csv\", index=False)\n",
    "\n",
    "# 8. GrÃ¡fico comparativo MAE promedio\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio7.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OTRO HORRIBLE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Dataset base\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Listado de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Cargar clusters\n",
    "clusters = pd.read_csv(\"dtw_clusters.csv\")\n",
    "\n",
    "# 4. Salidas\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "    datos['tn_lag1'] = datos['tn'].shift(1)\n",
    "    datos['tn_lag2'] = datos['tn'].shift(2)\n",
    "    datos['tn_roll3'] = datos['tn'].rolling(window=3).mean()\n",
    "    datos.dropna(inplace=True)\n",
    "\n",
    "    cluster = clusters.loc[clusters['product_id'] == prod, 'cluster'].values[0] if prod in clusters['product_id'].values else -1\n",
    "    datos['cluster'] = cluster\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    features = ['mes', 'tn_lag1', 'tn_lag2', 'tn_roll3', 'cluster']\n",
    "    X_train = train[features]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[features]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2, y_train.iloc[-1], y_train.iloc[-2], y_train.tail(3).mean(), cluster]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        input_feb = pd.DataFrame([[2, y_train.iloc[-1], y_train.iloc[-2], y_train.tail(3).mean(), cluster]], columns=features)\n",
    "        preds['lgbm'] = lgb_model.predict(input_feb)[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict(input_feb)[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon (modelos potentes)\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            enable_ensemble=False,\n",
    "            time_limit=5*60,\n",
    "            hyperparameters={\n",
    "                \"DeepAR\": {},\n",
    "                \"MLP\": {},\n",
    "                \"TemporalFusionTransformer\": {},\n",
    "                \"AutoARIMA\": {},\n",
    "                \"Naive\": {},\n",
    "                \"ETS\": {}\n",
    "            }\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback mÃ¡s fino\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos = datos[datos['periodo'] < '2020-01-01']\n",
    "    ultimos_validos = ultimos[ultimos['tn'] > 0].tail(3)\n",
    "    pred_fallback = ultimos_validos['tn'].mean() if not ultimos_validos.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback media Ãºltimos 3 positivos = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar salidas\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_ajustado.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo.csv\", index=False)\n",
    "with open(\"log_modelos.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# 8. GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dcf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AJUSTE REGULARIZACIÃ“N POR STOCK (ENTENDIENDO QUE EL SOBRESTOCK PUEDE SER UN PROBLEMA DE DESCENSO DE LA DEMANDA)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset principal\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Cargar stock final por producto\n",
    "stocks = pd.read_csv(\"tb_stocks.txt\", sep=\"\\t\")  # columnas: product_id, stock\n",
    "stock_dict = stocks.set_index(\"product_id\")[\"stock_final\"].to_dict()\n",
    "\n",
    "# 4. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "alpha = 0.05  # factor de penalizaciÃ³n por stock\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, lr.predict(X_val))\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        preds['arima'] = modelo_arima.forecast(steps=5)[-1]\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, lgb_model.predict(X_val))\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, xgb_model.predict(X_val))\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie.rename(columns={'periodo': 'timestamp'}, inplace=True)\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=5*60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "\n",
    "    # Ajuste por stock\n",
    "    stock_val = stock_dict.get(prod, 0)\n",
    "    pred_final_ajustada = pred_final / (1 + alpha * stock_val)\n",
    "\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final_ajustada})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}, stock = {stock_val}, pred ajustada = {pred_final_ajustada:.2f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    stock_val = stock_dict.get(prod, 0)\n",
    "    pred_final_ajustada = pred_fallback / (1 + alpha * stock_val)\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final_ajustada})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}, stock = {stock_val}, pred ajustada = {pred_final_ajustada:.2f}\")\n",
    "\n",
    "# Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_ajustadasstock.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelostock.csv\", index=False)\n",
    "with open(\"log_modelos_stock.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# GrÃ¡fico\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fa294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando productos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780/780 [02:06<00:00,  6.17it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cambio fuente de datos por datos_finales.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import contextlib\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ðŸ”‡ Context manager para silenciar outputs de LightGBM\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# 1. Cargar dataset final con features y stocks\n",
    "df = pd.read_csv(\"datos_finales.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'])\n",
    "\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados, log, maes_resumen = [], [], []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "# 4. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes, preds = {}, {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM (muteado)\n",
    "    try:\n",
    "        with suppress_output():\n",
    "            lgb_model = lgb.LGBMRegressor(\n",
    "                n_estimators=834,\n",
    "                learning_rate=0.06449926163783713,\n",
    "                max_depth=13,\n",
    "                num_leaves=197,\n",
    "                min_data_in_leaf=208,\n",
    "                min_child_weight=3.7932779938198546,\n",
    "                subsample=0.7032151245633396,\n",
    "                subsample_freq=7,\n",
    "                colsample_bytree=0.9893937066314805,\n",
    "                colsample_bynode=0.8148358693555268,\n",
    "                reg_alpha=4.962755134948597,\n",
    "                reg_lambda=3.8191748367071927,\n",
    "                max_bin=512,\n",
    "                min_split_gain=0.006311109685921704,\n",
    "                cat_smooth=49.82693114488869,\n",
    "                random_state=42,\n",
    "                boosting_type='dart',\n",
    "                verbosity=-1,\n",
    "                linear_tree=True\n",
    "            )\n",
    "            lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=5*60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto8.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo8.csv\", index=False)\n",
    "with open(\"log_modelos8.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# 8. GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81258173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validaciÃ³n ampliada a al 20% de los perÃ­odos. SIN AGOSTO NI EN TRAIN NI TEST. 0.333 en el public\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Context manager para silenciar outputs de LightGBM\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# 3. Inicializar salida\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "\n",
    "# 4. Carpeta autogluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# 5. Loop por producto\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-05-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-05-01'),\n",
    "        pd.Timestamp('2019-06-01'),\n",
    "        pd.Timestamp('2019-07-01'),\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01'),\n",
    "        pd.Timestamp('2019-12-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. RegresiÃ³n lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# 6. Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# 7. Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto9.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo9.csv\", index=False)\n",
    "with open(\"log_modelos9.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# 8. GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087659ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mejor rendimiento en public, modificando regresiÃ³n x los mÃ¡gicos del profesor. 0.289 en public\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "# 2. Lista fija de productos mÃ¡gicos\n",
    "magicos = [20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021,\n",
    "           20026, 20028, 20035, 20039, 20042, 20044, 20045, 20046,\n",
    "           20049, 20051, 20052, 20053, 20055, 20008, 20001, 20017,\n",
    "           20086, 20180, 20193, 20320, 20532, 20612, 20637, 20807, 20838]\n",
    "\n",
    "# 3. Cargar listado de productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # 1. Reemplazo: regresiÃ³n basada en productos mÃ¡gicos\n",
    "    try:\n",
    "        # Construir X_train con mÃ¡gicos en 2018\n",
    "        X_train = df_pivot[df_pivot['periodo'].between('2018-01-01', '2018-12-31')][magicos]\n",
    "        X_train = X_train.T\n",
    "        X_train.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "\n",
    "        # Variable objetivo: tn de febrero 2019 del producto\n",
    "        y_target = df[(df['product_id'] == prod) & (df['periodo'] == '2019-02-01')]['tn']\n",
    "        if y_target.empty:\n",
    "            raise ValueError(\"Sin dato objetivo para febrero 2019\")\n",
    "        y_train = y_target.values\n",
    "\n",
    "        # X_pred: mÃ¡gicos en 2019 para predecir febrero 2020\n",
    "        X_pred = df_pivot[df_pivot['periodo'].between('2019-01-01', '2019-12-31')][magicos]\n",
    "        X_pred = X_pred.T\n",
    "        X_pred.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        pred_feb2020 = lr.predict(X_pred)[0]\n",
    "\n",
    "        # ValidaciÃ³n: usar mÃ¡gicos 2018 â†’ febrero 2019 vs real\n",
    "        maes['regresion'] = mean_absolute_error(y_train, lr.predict(X_train))\n",
    "        preds['regresion'] = pred_feb2020\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # 2. ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(val['tn'].values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        X_train_simple = train[['mes']]\n",
    "        y_train_simple = train['tn']\n",
    "        X_val_simple = val[['mes']]\n",
    "        y_val_simple = val['tn']\n",
    "\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train_simple, y_train_simple)\n",
    "        y_pred = lgb_model.predict(X_val_simple)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val_simple, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # 4. XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train_simple, y_train_simple)\n",
    "        y_pred = xgb_model.predict(X_val_simple)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val_simple, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # 5. AutoGluon\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(val['tn'], val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Fallback para productos no predichos\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# Guardar resultados\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto10.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo10.csv\", index=False)\n",
    "with open(\"log_modelos10.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# GrÃ¡fico local\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# 1. Cargar dataset\n",
    "print(\"Cargando datos...\")\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "# 2. Productos mÃ¡gicos\n",
    "magicos = [20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021, 20026, 20028,\n",
    "           20035, 20039, 20042, 20044, 20045, 20046, 20049, 20051, 20052, 20053,\n",
    "           20055, 20008, 20001, 20017, 20086, 20180, 20193, 20320, 20532, 20612,\n",
    "           20637, 20807, 20838]\n",
    "\n",
    "# 3. Cargar productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "resultados, log, maes_resumen = [], [], []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'), pd.Timestamp('2019-10-01'), pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    maes, preds = {}, {}\n",
    "\n",
    "    # RegresiÃ³n con mÃ¡gicos\n",
    "    try:\n",
    "        X_train = df_pivot[df_pivot['periodo'].between('2018-01-01', '2018-12-31')][magicos].T\n",
    "        X_train.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "        X_pred = df_pivot[df_pivot['periodo'].between('2019-01-01', '2019-12-31')][magicos].T\n",
    "        X_pred.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "        y_target = df[(df['product_id'] == prod) & (df['periodo'] == '2019-02-01')]['tn']\n",
    "        if y_target.empty or X_train.isnull().any().any() or X_pred.isnull().any().any(): raise ValueError()\n",
    "        lr = LinearRegression().fit(X_train, y_target.values)\n",
    "        maes['regresion'] = mean_absolute_error(y_target.values, lr.predict(X_train))\n",
    "        preds['regresion'] = lr.predict(X_pred)[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1,1,1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(val['tn'].values, y_pred.values)\n",
    "        preds['arima'] = modelo_arima.forecast(steps=5)[-1]\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # LightGBM\n",
    "    try:\n",
    "        X_train_lgb, y_train_lgb = train[['mes']], train['tn']\n",
    "        X_val_lgb, y_val_lgb = val[['mes']], val['tn']\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.0645,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            subsample=0.7032,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9894,\n",
    "            colsample_bynode=0.8148,\n",
    "            reg_alpha=4.9628,\n",
    "            reg_lambda=3.8192,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.0063,\n",
    "            cat_smooth=49.8269,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True,\n",
    "            #min_child_samples=20\n",
    "        )\n",
    "        lgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "        y_pred = lgb_model.predict(X_val_lgb)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val_lgb, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # XGBoost\n",
    "    try:\n",
    "        if len(X_train_lgb) >= 2:\n",
    "            xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "            xgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "            y_pred = xgb_model.predict(X_val_lgb)\n",
    "            maes['xgboost'] = mean_absolute_error(y_val_lgb, y_pred)\n",
    "            preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "        else:\n",
    "            maes['xgboost'] = np.inf\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # AutoGluon\n",
    "    try:\n",
    "        if len(train) >= 4:\n",
    "            df_serie = train[['periodo', 'tn']].copy()\n",
    "            df_serie['item_id'] = str(prod)\n",
    "            df_serie = df_serie.rename(columns={'periodo': 'timestamp'})[['item_id', 'timestamp', 'tn']]\n",
    "            ts_data = TimeSeriesDataFrame.from_data_frame(df_serie, id_column='item_id', timestamp_column='timestamp').fill_missing_values()\n",
    "            predictor = TimeSeriesPredictor(prediction_length=5, target='tn', freq='MS', eval_metric='MASE', path=f\"autogluon_temp_ts/{prod}\", verbosity=0)\n",
    "            predictor.fit(ts_data, num_val_windows=2, time_limit=5*60, enable_ensemble=False, hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}})\n",
    "            forecast = predictor.predict(ts_data)\n",
    "            val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01','2019-10-01','2019-11-01']]\n",
    "            maes['autogluon'] = mean_absolute_error(val['tn'], val_preds)\n",
    "            preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        else:\n",
    "            maes['autogluon'] = np.inf\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': preds[mejor_modelo]})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion','arima','lgbm','xgboost','autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto11.csv\", index=False)\n",
    "pd.DataFrame(maes_resumen).sort_values(\"product_id\").to_csv(\"maes_por_modelo11.csv\", index=False)\n",
    "with open(\"log_modelos11.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# GrÃ¡fico MAE promedio\n",
    "maes_df = pd.DataFrame(maes_resumen)\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# 1. Cargar dataset\n",
    "print(\"Cargando datos...\")\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "# 2. Productos mÃ¡gicos\n",
    "magicos = [20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021, 20026, 20028,\n",
    "           20035, 20039, 20042, 20044, 20045, 20046, 20049, 20051, 20052, 20053,\n",
    "           20055, 20008, 20001, 20017, 20086, 20180, 20193, 20320, 20532, 20612,\n",
    "           20637, 20807, 20838]\n",
    "\n",
    "# Entrenar regresion global con magicos\n",
    "m2018 = df_pivot[df_pivot[\"periodo\"].between(\"2018-01-01\", \"2018-12-31\")][magicos].T\n",
    "m2018.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "feb2019 = df_pivot[df_pivot[\"periodo\"] == \"2019-02-01\"][magicos].T\n",
    "feb2019.columns = [\"target\"]\n",
    "regresion_magicos = LinearRegression()\n",
    "regresion_magicos.fit(m2018, feb2019[\"target\"])\n",
    "\n",
    "# 3. Cargar productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "resultados, log, maes_resumen = [], [], []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'), pd.Timestamp('2019-10-01'), pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    maes, preds = {}, {}\n",
    "\n",
    "    # Aplicar regresion global con magicos\n",
    "    try:\n",
    "        m2019 = df_pivot[df_pivot[\"periodo\"].between(\"2019-01-01\", \"2019-12-31\")][magicos].T\n",
    "        m2019.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "        if m2019.isnull().any().any(): raise ValueError()\n",
    "        x_pred = m2019.values.reshape(1, -1)\n",
    "        pred_feb2020 = regresion_magicos.predict(x_pred)[0]\n",
    "        pred_feb2019 = regresion_magicos.predict(m2018)\n",
    "        maes[\"regresion\"] = mean_absolute_error(feb2019[\"target\"], pred_feb2019)\n",
    "        preds[\"regresion\"] = pred_feb2020\n",
    "    except:\n",
    "        maes[\"regresion\"] = np.inf\n",
    "\n",
    "    # ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1,1,1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(val['tn'].values, y_pred.values)\n",
    "        preds['arima'] = modelo_arima.forecast(steps=5)[-1]\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # LightGBM\n",
    "    try:\n",
    "        X_train_lgb, y_train_lgb = train[['mes']], train['tn']\n",
    "        X_val_lgb, y_val_lgb = val[['mes']], val['tn']\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.0645,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            subsample=0.7032,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9894,\n",
    "            colsample_bynode=0.8148,\n",
    "            reg_alpha=4.9628,\n",
    "            reg_lambda=3.8192,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.0063,\n",
    "            cat_smooth=49.8269,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "        y_pred = lgb_model.predict(X_val_lgb)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val_lgb, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # XGBoost\n",
    "    try:\n",
    "        if len(X_train_lgb) >= 2 and y_train_lgb.nunique() > 1:\n",
    "\n",
    "            xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "            xgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "            y_pred = xgb_model.predict(X_val_lgb)\n",
    "            maes['xgboost'] = mean_absolute_error(y_val_lgb, y_pred)\n",
    "            preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "        else:\n",
    "            maes['xgboost'] = np.inf\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # AutoGluon\n",
    "    try:\n",
    "        if len(train) >= 4 and train[\"tn\"].gt(0).sum() >= 3:\n",
    "\n",
    "            df_serie = train[['periodo', 'tn']].copy()\n",
    "            df_serie['item_id'] = str(prod)\n",
    "            df_serie = df_serie.rename(columns={'periodo': 'timestamp'})[['item_id', 'timestamp', 'tn']]\n",
    "            ts_data = TimeSeriesDataFrame.from_data_frame(df_serie, id_column='item_id', timestamp_column='timestamp').fill_missing_values()\n",
    "            predictor = TimeSeriesPredictor(prediction_length=5, target='tn', freq='MS', eval_metric='MASE', path=f\"autogluon_temp_ts/{prod}\", verbosity=0)\n",
    "            predictor.fit(ts_data, num_val_windows=2, time_limit=5*60, enable_ensemble=False, hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}})\n",
    "            forecast = predictor.predict(ts_data)\n",
    "            val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01','2019-10-01','2019-11-01']]\n",
    "            maes['autogluon'] = mean_absolute_error(val['tn'], val_preds)\n",
    "            preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "        else:\n",
    "            maes['autogluon'] = np.inf\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': preds[mejor_modelo]})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion','arima','lgbm','xgboost','autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto12.csv\", index=False)\n",
    "pd.DataFrame(maes_resumen).sort_values(\"product_id\").to_csv(\"maes_por_modelo12.csv\", index=False)\n",
    "with open(\"log_modelos12.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# GrÃ¡fico MAE promedio\n",
    "maes_df = pd.DataFrame(maes_resumen)\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE Promedio por Modelo (Sep-Nov 2019)\")\n",
    "plt.ylabel(\"MAE promedio\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grafico_mae_promedio.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# 1. Cargar dataset\n",
    "print(\"Cargando datos...\")\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "# 2. Productos mÃ¡gicos\n",
    "magicos = [20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021, 20026, 20028,\n",
    "           20035, 20039, 20042, 20044, 20045, 20046, 20049, 20051, 20052, 20053,\n",
    "           20055, 20008, 20001, 20017, 20086, 20180, 20193, 20320, 20532, 20612,\n",
    "           20637, 20807, 20838]\n",
    "\n",
    "# Entrenar regresion global con magicos\n",
    "m2018 = df_pivot[df_pivot[\"periodo\"].between(\"2018-01-01\", \"2018-12-31\")][magicos].T\n",
    "m2018.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "feb2019 = df_pivot[df_pivot[\"periodo\"] == \"2019-02-01\"][magicos].T\n",
    "feb2019.columns = [\"target\"]\n",
    "regresion_magicos = LinearRegression()\n",
    "regresion_magicos.fit(m2018, feb2019[\"target\"])\n",
    "\n",
    "# 3. Cargar productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "resultados, log, maes_resumen = [], [], []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'), pd.Timestamp('2019-10-01'), pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12:\n",
    "        continue\n",
    "\n",
    "    if val.empty:\n",
    "        maes = {\n",
    "            \"regresion\": np.inf,\n",
    "            \"xgboost\": np.inf,\n",
    "            \"autogluon\": np.inf,\n",
    "            \"arima\": np.inf,\n",
    "            \"lgbm\": np.inf\n",
    "        }\n",
    "        resultados.append({'product_id': prod, 'tn_predicho': 0})\n",
    "        productos_predichos.add(prod)\n",
    "        log.append(f\"Producto {prod}: sin datos de validaciÃ³n â†’ todos los modelos = inf\")\n",
    "        mae_row = {'product_id': prod}\n",
    "        for modelo in ['regresion','arima','lgbm','xgboost','autogluon']:\n",
    "            mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "        maes_resumen.append(mae_row)    \n",
    "        continue\n",
    "\n",
    "    maes, preds = {}, {}\n",
    "\n",
    "    # Ignorar productos magicos en estos modelos\n",
    "    if prod in magicos:\n",
    "        maes['regresion'] = np.inf\n",
    "        maes['xgboost'] = np.inf\n",
    "        maes['autogluon'] = np.inf\n",
    "    else:\n",
    "        # Aplicar regresion global con magicos\n",
    "        try:\n",
    "            m2019 = df_pivot[df_pivot[\"periodo\"].between(\"2019-01-01\", \"2019-12-31\")][magicos].T\n",
    "            m2019.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "            if m2019.isnull().any().any(): raise ValueError()\n",
    "            x_pred = m2019.values.reshape(1, -1)\n",
    "            pred_feb2020 = regresion_magicos.predict(x_pred)[0]\n",
    "            pred_feb2019 = regresion_magicos.predict(m2018)\n",
    "            maes[\"regresion\"] = mean_absolute_error(feb2019[\"target\"], pred_feb2019)\n",
    "            preds[\"regresion\"] = pred_feb2020\n",
    "        except:\n",
    "            maes[\"regresion\"] = np.inf\n",
    "\n",
    "        # XGBoost\n",
    "        try:\n",
    "            X_train_lgb, y_train_lgb = train[['mes']], train['tn']\n",
    "            if len(X_train_lgb) >= 2 and y_train_lgb.nunique() > 1:\n",
    "                xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "                xgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "                y_pred = xgb_model.predict(val[['mes']])\n",
    "                maes['xgboost'] = mean_absolute_error(val['tn'], y_pred)\n",
    "                preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "            else:\n",
    "                maes['xgboost'] = np.inf\n",
    "        except:\n",
    "            maes['xgboost'] = np.inf\n",
    "\n",
    "        # AutoGluon\n",
    "        try:\n",
    "            if len(train) >= 4 and train['tn'].gt(0).sum() >= 3:\n",
    "                df_serie = train[['periodo', 'tn']].copy()\n",
    "                df_serie['item_id'] = str(prod)\n",
    "                df_serie = df_serie.rename(columns={'periodo': 'timestamp'})[['item_id', 'timestamp', 'tn']]\n",
    "                ts_data = TimeSeriesDataFrame.from_data_frame(df_serie, id_column='item_id', timestamp_column='timestamp').fill_missing_values()\n",
    "                predictor = TimeSeriesPredictor(prediction_length=5, target='tn', freq='MS', eval_metric='MASE', path=f\"autogluon_temp_ts/{prod}\", verbosity=0)\n",
    "                predictor.fit(ts_data, num_val_windows=2, time_limit=5*60, enable_ensemble=False, hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}})\n",
    "                forecast = predictor.predict(ts_data)\n",
    "                val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01','2019-10-01','2019-11-01']]\n",
    "                maes['autogluon'] = mean_absolute_error(val['tn'], val_preds)\n",
    "                preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "            else:\n",
    "                maes['autogluon'] = np.inf\n",
    "        except:\n",
    "            maes['autogluon'] = np.inf\n",
    "\n",
    "    # ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1,1,1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(val['tn'].values, y_pred.values)\n",
    "        preds['arima'] = modelo_arima.forecast(steps=5)[-1]\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # LightGBM\n",
    "    try:\n",
    "        X_train_lgb, y_train_lgb = train[['mes']], train['tn']\n",
    "        X_val_lgb, y_val_lgb = val[['mes']], val['tn']\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.0645,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            subsample=0.7032,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9894,\n",
    "            colsample_bynode=0.8148,\n",
    "            reg_alpha=4.9628,\n",
    "            reg_lambda=3.8192,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.0063,\n",
    "            cat_smooth=49.8269,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "        y_pred = lgb_model.predict(X_val_lgb)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val_lgb, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': preds.get(mejor_modelo, 0)})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion','arima','lgbm','xgboost','autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Fallback\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# Guardar\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto14.csv\", index=False)\n",
    "pd.DataFrame(maes_resumen).sort_values(\"product_id\").to_csv(\"maes_por_modelo14.csv\", index=False)\n",
    "with open(\"log_modelos14.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "# GrÃ¡fico MAE promedio\n",
    "maes_df = pd.DataFrame(maes_resumen)\n",
    "maes_long = maes_df.melt(id_vars='product_id', value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "                         var_name='modelo', value_name='mae')\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a71fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "# Verificar archivos necesarios\n",
    "if not os.path.exists(\"sell-in.txt\"):\n",
    "    raise FileNotFoundError(\"No se encontrÃ³ 'sell-in.txt'\")\n",
    "if not os.path.exists(\"product_id_apredecir201912.TXT\"):\n",
    "    raise FileNotFoundError(\"No se encontrÃ³ 'product_id_apredecir201912.TXT'\")\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "try:\n",
    "    df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error al leer 'sell-in.txt': {e}\")\n",
    "\n",
    "try:\n",
    "    df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error al convertir la columna 'periodo': {e}\")\n",
    "\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "magicos = [20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021, 20026, 20028,\n",
    "           20035, 20039, 20042, 20044, 20045, 20046, 20049, 20051, 20052, 20053,\n",
    "           20055, 20008, 20001, 20017, 20086, 20180, 20193, 20320, 20532, 20612,\n",
    "           20637, 20807, 20838]\n",
    "\n",
    "# Filtrar solo los product_id mÃ¡gicos disponibles en df_pivot\n",
    "magicos_disponibles = [m for m in magicos if m in df_pivot.columns]\n",
    "\n",
    "# Preparar matriz para regresiÃ³n global con mÃ¡gicos (2018)\n",
    "m2018 = df_pivot[df_pivot[\"periodo\"].between(\"2018-01-01\", \"2018-12-31\")][magicos_disponibles].T\n",
    "if m2018.shape[1] != 12:\n",
    "    raise RuntimeError(\"No hay 12 meses de datos para productos mÃ¡gicos en 2018\")\n",
    "m2018.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "\n",
    "feb2019 = df_pivot[df_pivot[\"periodo\"] == \"2019-02-01\"][magicos_disponibles].T\n",
    "feb2019.columns = [\"target\"]\n",
    "\n",
    "regresion_magicos = LinearRegression()\n",
    "regresion_magicos.fit(m2018, feb2019[\"target\"])\n",
    "\n",
    "# Cargar productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "resultados, log, maes_resumen = [], [], []\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "productos_predichos = set()\n",
    "\n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'), pd.Timestamp('2019-10-01'), pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12:\n",
    "        log.append(f\"Producto {prod}: menos de 12 perÃ­odos de entrenamiento, resultado fallback.\")\n",
    "        maes = {k: np.inf for k in ['regresion','arima','lgbm','xgboost','autogluon']}\n",
    "        resultados.append({'product_id': prod, 'tn_predicho': 0})\n",
    "        mae_row = {'product_id': prod, **{f'mae_{k}': v for k,v in maes.items()}}\n",
    "        maes_resumen.append(mae_row)\n",
    "        productos_predichos.add(prod)\n",
    "        continue\n",
    "\n",
    "    if val.empty:\n",
    "        maes = {k: np.inf for k in ['regresion','arima','lgbm','xgboost','autogluon']}\n",
    "        resultados.append({'product_id': prod, 'tn_predicho': 0})\n",
    "        log.append(f\"Producto {prod}: sin datos de validaciÃ³n â†’ todos los modelos = inf\")\n",
    "        mae_row = {'product_id': prod, **{f'mae_{k}': v for k,v in maes.items()}}\n",
    "        maes_resumen.append(mae_row)\n",
    "        productos_predichos.add(prod)\n",
    "        continue\n",
    "\n",
    "    maes, preds = {}, {}\n",
    "\n",
    "    if prod in magicos_disponibles:\n",
    "        maes['regresion'] = np.inf\n",
    "        maes['xgboost'] = np.inf\n",
    "        maes['autogluon'] = np.inf\n",
    "    else:\n",
    "        try:\n",
    "            m2019 = df_pivot[df_pivot[\"periodo\"].between(\"2019-01-01\", \"2019-12-31\")][magicos_disponibles].T\n",
    "            if m2019.shape[1] != 12 or m2019.isnull().any().any():\n",
    "                raise ValueError(\"Datos incompletos o NaN para productos mÃ¡gicos en 2019\")\n",
    "            m2019.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "            x_pred = m2019.values.reshape(1, -1)\n",
    "            pred_feb2020 = regresion_magicos.predict(x_pred)[0]\n",
    "            pred_feb2019 = regresion_magicos.predict(m2018)\n",
    "            maes[\"regresion\"] = mean_absolute_error(feb2019[\"target\"], pred_feb2019)\n",
    "            preds[\"regresion\"] = pred_feb2020\n",
    "        except Exception as e:\n",
    "            maes[\"regresion\"] = np.inf\n",
    "            log.append(f\"Producto {prod}: error regresiÃ³n global: {str(e)}\")\n",
    "\n",
    "        try:\n",
    "            X_train_lgb, y_train_lgb = train[['mes']], train['tn']\n",
    "            if len(X_train_lgb) >= 2 and y_train_lgb.nunique() > 1:\n",
    "                xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "                xgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "                y_pred = xgb_model.predict(val[['mes']])\n",
    "                maes['xgboost'] = mean_absolute_error(val['tn'], y_pred)\n",
    "                preds['xgboost'] = xgb_model.predict(np.array([[2]]))[0]\n",
    "            else:\n",
    "                maes['xgboost'] = np.inf\n",
    "        except Exception as e:\n",
    "            maes['xgboost'] = np.inf\n",
    "            log.append(f\"Producto {prod}: error XGBoost: {str(e)}\")\n",
    "\n",
    "        try:\n",
    "            if len(train) >= 4 and train['tn'].gt(0).sum() >= 3:\n",
    "                df_serie = train[['periodo', 'tn']].copy()\n",
    "                df_serie['item_id'] = str(prod)\n",
    "                df_serie = df_serie.rename(columns={'periodo': 'timestamp'})[['item_id', 'timestamp', 'tn']]\n",
    "                ts_data = TimeSeriesDataFrame.from_data_frame(df_serie, id_column='item_id', timestamp_column='timestamp').fill_missing_values()\n",
    "                with suppress_output():\n",
    "                    predictor = TimeSeriesPredictor(\n",
    "                        prediction_length=5, target='tn', freq='MS', eval_metric='MASE',\n",
    "                        path=f\"autogluon_temp_ts/{prod}\", verbosity=0\n",
    "                    )\n",
    "                    predictor.fit(ts_data, num_val_windows=2, time_limit=5*60, enable_ensemble=False,\n",
    "                                  hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}})\n",
    "                forecast = predictor.predict(ts_data)\n",
    "                val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01','2019-10-01','2019-11-01'] if (str(prod), pd.Timestamp(d)) in forecast.index]\n",
    "                if len(val_preds) == len(val):\n",
    "                    maes['autogluon'] = mean_absolute_error(val['tn'], val_preds)\n",
    "                else:\n",
    "                    maes['autogluon'] = np.inf\n",
    "                preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean'] if (str(prod), pd.Timestamp(\"2020-02-01\")) in forecast.index else 0\n",
    "            else:\n",
    "                maes['autogluon'] = np.inf\n",
    "        except Exception as e:\n",
    "            maes['autogluon'] = np.inf\n",
    "            log.append(f\"Producto {prod}: error AutoGluon: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1,1,1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(val['tn'].values, y_pred.values)\n",
    "        preds['arima'] = modelo_arima.forecast(steps=5)[-1]\n",
    "    except Exception as e:\n",
    "        maes['arima'] = np.inf\n",
    "        log.append(f\"Producto {prod}: error ARIMA: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        X_train_lgb, y_train_lgb = train[['mes']], train['tn']\n",
    "        X_val_lgb, y_val_lgb = val[['mes']], val['tn']\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.0645,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            subsample=0.7032,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9894,\n",
    "            colsample_bynode=0.8148,\n",
    "            reg_alpha=4.9628,\n",
    "            reg_lambda=3.8192,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.0063,\n",
    "            cat_smooth=49.8269,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1\n",
    "        )\n",
    "        lgb_model.fit(X_train_lgb, y_train_lgb)\n",
    "        y_pred = lgb_model.predict(X_val_lgb)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val_lgb, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict(np.array([[2]]))[0]\n",
    "    except Exception as e:\n",
    "        maes['lgbm'] = np.inf\n",
    "        log.append(f\"Producto {prod}: error LightGBM: {str(e)}\")\n",
    "\n",
    "    if maes:\n",
    "        mejor_modelo = min(maes, key=maes.get)\n",
    "        resultados.append({'product_id': prod, 'tn_predicho': preds.get(mejor_modelo, 0)})\n",
    "        productos_predichos.add(prod)\n",
    "        log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "        mae_row = {'product_id': prod}\n",
    "        for modelo in ['regresion','arima','lgbm','xgboost','autogluon']:\n",
    "            mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "        maes_resumen.append(mae_row)\n",
    "\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio Ãºltimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto15.csv\", index=False)\n",
    "pd.DataFrame(maes_resumen).sort_values(\"product_id\").to_csv(\"maes_por_modelo15.csv\", index=False)\n",
    "\n",
    "with open(\"log_modelos15.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "maes_df = pd.DataFrame(maes_resumen)\n",
    "maes_long = maes_df.melt(\n",
    "    id_vars='product_id',\n",
    "    value_vars=[col for col in maes_df.columns if col.startswith('mae_')],\n",
    "    var_name='modelo', value_name='mae'\n",
    ")\n",
    "maes_long['modelo'] = maes_long['modelo'].str.replace('mae_', '')\n",
    "mae_promedios = maes_long.groupby('modelo')['mae'].mean().reset_index().sort_values('mae')\n",
    "\n",
    "# Opcional: mostrar grÃ¡fico de MAE promedio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=mae_promedios, x='modelo', y='mae')\n",
    "plt.title(\"MAE promedio por modelo\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"Modelo\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predprod1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
