{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMER SCRIPT: REALIZA UNA REGRESIÓN LINEAL SIMPLE\n",
    "# Este script realiza una regresión simple propuesta en clase.\n",
    "# Usa como variables los \"mágicos\", que son ciertos productos seleccionados para el análisis.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carga del dataset de ventas\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "\n",
    "# Se agrupa por periodo y producto, sumando las toneladas vendidas (tn)\n",
    "df = df.groupby(by=[\"periodo\", \"product_id\"]).agg({\"tn\": \"sum\"}).reset_index()\n",
    "\n",
    "# Se convierte la columna 'periodo' a formato datetime\n",
    "df[\"periodo\"] = pd.to_datetime(df[\"periodo\"], format=\"%Y%m\")\n",
    "\n",
    "# Reestructuración del DataFrame: cada fila es un mes, cada columna un product_id, valores = tn\n",
    "df_pivot = df.pivot(index=\"periodo\", columns=\"product_id\", values=\"tn\").reset_index()\n",
    "\n",
    "# 5. Lista de productos mágicos seleccionados para el análisis (más la columna 'periodo')\n",
    "magicos = [ \n",
    "    \"periodo\", 20002, 20003, 20006, 20010, 20011, 20018, 20019, 20021,\n",
    "    20026, 20028, 20035, 20039, 20042, 20044, 20045, 20046, 20049,\n",
    "    20051, 20052, 20053, 20055, 20008, 20001, 20017, 20086, 20180,\n",
    "    20193, 20320, 20532, 20612, 20637, 20807, 20838\n",
    "]\n",
    "\n",
    "# Se construye el conjunto de entrenamiento (X_train) con datos del año 2018\n",
    "X_train = df_pivot[magicos].query(\"periodo >= '2018-01-01' & periodo <= '2018-12-31'\")\n",
    "X_train = X_train.T.iloc[1:]  # Transpone y elimina la fila 'periodo'\n",
    "X_train.columns = [f\"t-{11-k}\" for k in range(12)]  # Etiquetas de los 12 meses hacia atrás\n",
    "\n",
    "# Se construye el conjunto X_kgl con los datos de 2019, para predicción\n",
    "X_kgl = df_pivot.query(\"periodo >= '2019-01-01' & periodo <= '2019-12-31'\")\n",
    "X_kgl = X_kgl.T.iloc[1:]  # Transpone y elimina la fila 'periodo'\n",
    "X_kgl.columns = [f\"t-{11-k}\" for k in range(12)]\n",
    "\n",
    "# Se calcula el promedio por producto (fila) como baseline, por si faltan datos\n",
    "promedio = X_kgl.mean(axis=1).fillna(0)\n",
    "\n",
    "# Se crea el vector objetivo (y) usando las toneladas de febrero 2019\n",
    "y = df_pivot[magicos].query(\"periodo == '2019-02-01'\").T.iloc[1:]\n",
    "y.columns = [\"target\"]\n",
    "\n",
    "# Se eliminan productos que no tengan 12 meses de datos (tienen NaN)\n",
    "prod_menos12 = X_kgl.index[X_kgl.isna().sum(axis=1) > 0]\n",
    "X_kgl = X_kgl[~X_kgl.index.isin(prod_menos12)]  # Solo productos con info completa\n",
    "promedio_menos12 = promedio[prod_menos12]  # Guarda promedio para los que tienen datos faltantes\n",
    "\n",
    "# Se carga la lista de productos a predecir desde el archivo externo\n",
    "productos_ok = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/open-courses/austral2025-af91/labo3v/product_id_apredecir201912.txt\", \n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Se entrena un modelo de regresión lineal usando X_train e y\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train, y)\n",
    "\n",
    "# Se realiza la predicción para los productos que tienen datos completos\n",
    "pred = pd.DataFrame({\n",
    "    \"product_id\": X_kgl.index,\n",
    "    \"tn\": reg_model.predict(X_kgl).flatten()\n",
    "})\n",
    "\n",
    "# Para los productos faltantes, se completa con el promedio del año\n",
    "nuevas_filas = []\n",
    "for prod in productos_ok[\"product_id\"]:\n",
    "    if prod not in pred[\"product_id\"].values:\n",
    "        nuevas_filas.append({\n",
    "            \"product_id\": prod, \n",
    "            \"tn\": promedio[prod]\n",
    "        })\n",
    "\n",
    "# Se unen las predicciones del modelo con los productos completados por promedio\n",
    "pred = pd.concat([pred, pd.DataFrame(nuevas_filas)], ignore_index=True)\n",
    "\n",
    "# Se aseguran solo los productos requeridos en la lista final\n",
    "pred = pred[pred[\"product_id\"].isin(productos_ok[\"product_id\"])]\n",
    "\n",
    "# Exportación de las predicciones a un archivo CSV \n",
    "pred.to_csv(\"prediccion_reg_lineal.csv\", index=False, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGUNDO SCRIPT: AUTOGLUON\n",
    "#El mismo script que fue compartido en zulip por Fernando Raco, tal cual lo compartió él (solo unificado) \n",
    "# y que estuvimos viendo el clase, de probada funcionalidad. \n",
    "\n",
    "# 📦 1. Importar librerías\n",
    "import pandas as pd\n",
    "# 💬 Instalar AutoGluon si es necesario\n",
    "%pip install autogluon.timeseries\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "\n",
    "# 📄 2. Cargar datasets\n",
    "df_sellin = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df_productos = pd.read_csv(\"tb_productos.txt\", sep=\"\\t\")\n",
    "\n",
    "# 📄 Leer lista de productos a predecir\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    product_ids = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "    \n",
    "    # 🧹 3. Preprocesamiento\n",
    "# Convertir periodo a datetime\n",
    "df_sellin['timestamp'] = pd.to_datetime(df_sellin['periodo'], format='%Y%m')\n",
    "\n",
    "# Filtrar hasta dic 2019 y productos requeridos\n",
    "df_filtered = df_sellin[\n",
    "    (df_sellin['timestamp'] <= '2019-12-01') &\n",
    "    (df_sellin['product_id'].isin(product_ids))\n",
    "]\n",
    "\n",
    "# Agregar tn por periodo, cliente y producto\n",
    "df_grouped = df_filtered.groupby(['timestamp', 'customer_id', 'product_id'], as_index=False)['tn'].sum()\n",
    "\n",
    "# Agregar tn total por periodo y producto\n",
    "df_monthly_product = df_grouped.groupby(['timestamp', 'product_id'], as_index=False)['tn'].sum()\n",
    "\n",
    "# Agregar columna 'item_id' para AutoGluon\n",
    "df_monthly_product['item_id'] = df_monthly_product['product_id']\n",
    "\n",
    "# ⏰ 4. Crear TimeSeriesDataFrame\n",
    "ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    df_monthly_product,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "# Completar valores faltantes\n",
    "ts_data = ts_data.fill_missing_values()\n",
    "\n",
    "# ⚙️ 5. Definir y entrenar predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=2,\n",
    "    target='tn',\n",
    "    freq='MS'  # Frecuencia mensual (Month Start), \n",
    ")\n",
    "\n",
    "predictor.fit(ts_data, num_val_windows=2, time_limit=60*60)\n",
    "\n",
    "# 🔮 6. Generar predicción\n",
    "forecast = predictor.predict(ts_data)\n",
    "\n",
    "# Extraer predicción media y filtrar febrero 2020\n",
    "forecast_mean = forecast['mean'].reset_index()\n",
    "print(forecast_mean.columns)\n",
    "\n",
    "# Tomar solo item_id y la predicción 'mean'\n",
    "resultado = forecast['mean'].reset_index()[['item_id', 'mean']]\n",
    "resultado.columns = ['product_id', 'tn']\n",
    "\n",
    "# Filtrar solo febrero 2020\n",
    "resultado = forecast['mean'].reset_index()\n",
    "resultado = resultado[resultado['timestamp'] == '2020-02-01']\n",
    "\n",
    "# Renombrar columnas\n",
    "resultado = resultado[['item_id', 'mean']]\n",
    "resultado.columns = ['product_id', 'tn']\n",
    "\n",
    "\n",
    "# 💾 7. Guardar archivo\n",
    "resultado.to_csv(\"predicciones_febrero2020_fecha_01_07.csv\", index=False)\n",
    "resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TERCER SCRIPT: COMPARATIVA DE MODELOS\n",
    "# Este script ejecuta una comparativa de modelos, con validación ampliada a septiembre, octubre y noviembre. 0.260 en el public\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Cargar dataset\n",
    "df = pd.read_csv(\"sell-in.txt\", sep=\"\\t\")\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df = df.groupby(['product_id', 'periodo'])['tn'].sum().reset_index()\n",
    "\n",
    "# 2. Cargar listado fijo de productos\n",
    "with open(\"product_id_apredecir201912.TXT\", \"r\") as f:\n",
    "    productos = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "# Se inicializa la salida\n",
    "resultados = []\n",
    "log = []\n",
    "maes_resumen = []\n",
    "\n",
    "# Se crea carpeta par autogluon\n",
    "os.makedirs(\"autogluon_temp_ts\", exist_ok=True)\n",
    "\n",
    "productos_predichos = set()\n",
    "\n",
    "# Se filtran los datos del df, ordenándolos por fecha y extrayendo el mes como única feature.\n",
    "# Se dividen los datos en un conjunto de entrenamiento (todo antes de septiembre de 2019) y otro de validación (solo septiembre, octubre y noviembre de 2019)\n",
    "# Se preparan los datasets X_train y X_val usando únicamente el mes como variable explicativa, y y_train y y_val con las toneladas vendidas. \n",
    "for prod in tqdm(productos, desc=\"Procesando productos\"):\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    datos['mes'] = datos['periodo'].dt.month\n",
    "\n",
    "    train = datos[datos['periodo'] < '2019-09-01'].copy()\n",
    "    val = datos[datos['periodo'].isin([\n",
    "        pd.Timestamp('2019-09-01'),\n",
    "        pd.Timestamp('2019-10-01'),\n",
    "        pd.Timestamp('2019-11-01')\n",
    "    ])].copy()\n",
    "\n",
    "    if len(train) < 12 or val.empty:\n",
    "        continue\n",
    "\n",
    "    X_train = train[['mes']]\n",
    "    y_train = train['tn']\n",
    "    X_val = val[['mes']]\n",
    "    y_val = val['tn']\n",
    "\n",
    "    maes = {}\n",
    "    preds = {}\n",
    "\n",
    "    # Se aplica el primer modelo de Regresión lineal\n",
    "    try:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_val)\n",
    "        maes['regresion'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['regresion'] = lr.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['regresion'] = np.inf\n",
    "\n",
    "    # Se aplica el segundo modelo: ARIMA\n",
    "    try:\n",
    "        serie = train.set_index('periodo')['tn']\n",
    "        modelo_arima = ARIMA(serie, order=(1, 1, 1)).fit()\n",
    "        y_pred = modelo_arima.forecast(steps=3)\n",
    "        maes['arima'] = mean_absolute_error(y_val.values, y_pred.values)\n",
    "        feb_pred = modelo_arima.forecast(steps=5)[-1]\n",
    "        preds['arima'] = feb_pred\n",
    "    except:\n",
    "        maes['arima'] = np.inf\n",
    "\n",
    "    # Se aplica el tercer modelo, LightGBM. Los hiperparámetros son ajustados en función de una optimización previa con Optuna.\n",
    "    try:\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=834,\n",
    "            learning_rate=0.06449926163783713,\n",
    "            max_depth=13,\n",
    "            num_leaves=197,\n",
    "            min_data_in_leaf=208,\n",
    "            min_child_weight=3.7932779938198546,\n",
    "            subsample=0.7032151245633396,\n",
    "            subsample_freq=7,\n",
    "            colsample_bytree=0.9893937066314805,\n",
    "            colsample_bynode=0.8148358693555268,\n",
    "            reg_alpha=4.962755134948597,\n",
    "            reg_lambda=3.8191748367071927,\n",
    "            max_bin=512,\n",
    "            min_split_gain=0.006311109685921704,\n",
    "            cat_smooth=49.82693114488869,\n",
    "            random_state=42,\n",
    "            boosting_type='dart',\n",
    "            verbosity=-1,\n",
    "            linear_tree=True\n",
    "        )\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        y_pred = lgb_model.predict(X_val)\n",
    "        maes['lgbm'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['lgbm'] = lgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['lgbm'] = np.inf\n",
    "\n",
    "    # Se aplica el cuarto modelo: XGBoost\n",
    "    try:\n",
    "        xgb_model = xgb.XGBRegressor(verbosity=0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_val)\n",
    "        maes['xgboost'] = mean_absolute_error(y_val, y_pred)\n",
    "        preds['xgboost'] = xgb_model.predict([[2]])[0]\n",
    "    except:\n",
    "        maes['xgboost'] = np.inf\n",
    "\n",
    "    # Se aplica el quinto modelo, AutoGluon, tomando como base el script que se había probado funcional en clase.\n",
    "    try:\n",
    "        df_serie = train[['periodo', 'tn']].copy()\n",
    "        df_serie['item_id'] = str(prod)\n",
    "        df_serie = df_serie.rename(columns={'periodo': 'timestamp'})\n",
    "        df_serie = df_serie[['item_id', 'timestamp', 'tn']]\n",
    "\n",
    "        ts_data = TimeSeriesDataFrame.from_data_frame(\n",
    "            df_serie, id_column='item_id', timestamp_column='timestamp'\n",
    "        ).fill_missing_values()\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=5,\n",
    "            target='tn',\n",
    "            freq='MS',\n",
    "            eval_metric='MASE',\n",
    "            path=f\"autogluon_temp_ts/{prod}\",\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            ts_data,\n",
    "            num_val_windows=2,\n",
    "            time_limit=60,\n",
    "            enable_ensemble=False,\n",
    "            hyperparameters={\"ETS\": {}, \"AutoARIMA\": {}, \"Naive\": {}}\n",
    "        )\n",
    "\n",
    "        forecast = predictor.predict(ts_data)\n",
    "        val_preds = [forecast.loc[(str(prod), pd.Timestamp(d)), 'mean'] for d in ['2019-09-01', '2019-10-01', '2019-11-01']]\n",
    "        maes['autogluon'] = mean_absolute_error(y_val, val_preds)\n",
    "        preds['autogluon'] = forecast.loc[(str(prod), pd.Timestamp(\"2020-02-01\")), 'mean']\n",
    "    except:\n",
    "        maes['autogluon'] = np.inf\n",
    "\n",
    "   # Este bloque selecciona el mejor modelo, basándose en el error MAE.  \n",
    "    mejor_modelo = min(maes, key=maes.get)\n",
    "    pred_final = preds[mejor_modelo]\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_final})\n",
    "    productos_predichos.add(prod)\n",
    "    log.append(f\"Producto {prod}: mejor modelo = {mejor_modelo}, MAE sep-nov = {maes[mejor_modelo]:.4f}\")\n",
    "\n",
    "    mae_row = {'product_id': prod}\n",
    "    for modelo in ['regresion', 'arima', 'lgbm', 'xgboost', 'autogluon']:\n",
    "        mae_row[f'mae_{modelo}'] = maes.get(modelo, np.nan)\n",
    "    maes_resumen.append(mae_row)\n",
    "\n",
    "# Para los productos que no pudieron ser modelados en el paso anterior (por falta de datos o por error)\n",
    "# se aplica una estrategia de fallback para asegurar que todos los productos tengan una predicción.\n",
    "# Se calcula el promedio de las toneladas vendidas en los últimos 12 meses antes de septiembre\n",
    "productos_faltantes = set(productos) - productos_predichos\n",
    "for prod in productos_faltantes:\n",
    "    datos = df[df['product_id'] == prod].sort_values('periodo').copy()\n",
    "    ultimos_12 = datos[datos['periodo'] < '2020-01-01'].tail(12)\n",
    "    pred_fallback = ultimos_12['tn'].mean() if not ultimos_12.empty else 0\n",
    "    resultados.append({'product_id': prod, 'tn_predicho': pred_fallback})\n",
    "    log.append(f\"Producto {prod}: fallback promedio últimos 12 meses = {pred_fallback:.2f}\")\n",
    "\n",
    "# Se guardan los resultados en archivos csv \n",
    "pd.DataFrame(resultados).sort_values(\"product_id\").to_csv(\"predicciones_febrero2020_porproducto3.csv\", index=False)\n",
    "maes_df = pd.DataFrame(maes_resumen).sort_values(\"product_id\")\n",
    "maes_df.to_csv(\"maes_por_modelo.csv\", index=False)\n",
    "with open(\"log_modelos3.txt\", \"w\") as f:\n",
    "    for linea in log:\n",
    "        f.write(linea + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUARTO SCRIPT: ENSEMBLE DE RESULTADOS\n",
    "# Este script combina las predicciones de varios modelos para obtener una predicción final promediada. Se utilizan para ello\n",
    "# los acrchivos generados por los scripts anteriores. Y se combinan con un cuarto, que es el resultado del modelo elegido por Pablo Cablinski.\n",
    "# de ese modo se presenta un csv que es el elegido por el grupo para la entrega final del laboratorio. En el public performa 0.242\n",
    "\n",
    "# Cargar archivos\n",
    "df1 = pd.read_csv(\"predicciones_febrero2020_porproducto3.csv\")\n",
    "df2 = pd.read_csv(\"prediccion_reg_lineal.csv\")\n",
    "df3 = pd.read_csv(\"lgbm_predictions_median.csv\")\n",
    "df4 = pd.read_csv(\"predicciones_febrero2020_autogluonFernando.csv\")\n",
    "\n",
    "\n",
    "# Renombrar columnas para consistencia\n",
    "df1.columns = [\"product_id\", \"tn_predicho\"]\n",
    "df2.columns = [\"product_id\", \"tn_predicho\"]\n",
    "df3.columns = [\"product_id\", \"tn_predicho\"]\n",
    "df4.columns = [\"product_id\", \"tn_predicho\"]\n",
    "\n",
    "\n",
    "# Unir por product_id\n",
    "df_merge = df1.merge(df2, on=\"product_id\", suffixes=(\"_1\", \"_2\"))\n",
    "df_merge = df_merge.merge(df3, on=\"product_id\")\n",
    "df_merge.rename(columns={\"tn_predicho\": \"tn_predicho_3\"}, inplace=True)\n",
    "df_merge = df_merge.merge(df4, on=\"product_id\")\n",
    "df_merge.rename(columns={\"tn_predicho\": \"tn_predicho_4\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Calcular promedio\n",
    "df_merge[\"tn_predicho\"] = df_merge[\n",
    "    [\"tn_predicho_1\", \"tn_predicho_2\", \"tn_predicho_3\", \"tn_predicho_4\"]\n",
    "].mean(axis=1)\n",
    "\n",
    "# Mergear las predicciones del modelo\n",
    "df_merge[\"tn_predicho\"] \n",
    "\n",
    "# Seleccionar columnas finales\n",
    "df_final = df_merge[[\"product_id\", \"tn_predicho\"]]\n",
    "\n",
    "# Guardar archivo final\n",
    "df_final.to_csv(\"predicciones_promediadas_pw.csv\", index=False)\n",
    "print(\"Archivo guardado como predicciones_promediadas_pw.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predprod1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
